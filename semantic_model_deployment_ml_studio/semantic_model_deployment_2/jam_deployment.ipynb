{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***Dependencies***: Only necessary for initially running notebook code."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install azure-ai-ml\r\n",
        "%pip install -U transformers==4.20\r\n",
        "%pip install -U tensorflow==2.9\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1670236923611
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Connect to workspace***: Run this block of code when beginning session. This will connect the notebook to the workspace which contains all resources and artifacts needed for deploying the model. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\r\n",
        "from azure.ai.ml.entities import Model\r\n",
        "from azure.ai.ml.constants import AssetTypes\r\n",
        "from azure.ai.ml import MLClient\r\n",
        "from azure.identity import DefaultAzureCredential\r\n",
        "\r\n",
        "credential = DefaultAzureCredential()\r\n",
        "\r\n",
        "# connect to the workspace\r\n",
        "ml_client = None\r\n",
        "try:\r\n",
        "    ml_client = MLClient.from_config(credential)\r\n",
        "except Exception as ex:\r\n",
        "    print(ex)\r\n",
        "    # Enter details of your AzureML workspace\r\n",
        "    subscription_id = \"<SUBSCRIPTION_ID>\"\r\n",
        "    resource_group = \"<RESOURCE_GROUP>\"\r\n",
        "    workspace = \"<AZUREML_WORKSPACE_NAME>\"\r\n",
        "    ml_client = MLClient(credential, subscription_id, resource_group, workspace)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1672761083371
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Create environment***: This will create the containerised environment for deployment. It will take a docker image along with conda file, containing all necassary dependencies. To update dependencies, edit the conda.yml file and rerun code block below."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import Environment\r\n",
        "\r\n",
        "env_docker_conda = Environment(\r\n",
        "    image=\"mcr.microsoft.com/azureml/minimal-ubuntu20.04-py38-cpu-inference:latest\",\r\n",
        "    conda_file=\"src/conda.yml\",\r\n",
        "    name=\"jam_environment\",\r\n",
        "    description=\"Environment created from a Docker image plus Conda environment.\",\r\n",
        ")\r\n",
        "ml_client.environments.create_or_update(env_docker_conda)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1672759758019
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Create model ***: This will train the model and output two model files in src/deployment/model. The hf_model.h5 is the hugging face base model file and custom_bert.h5 is the fine-tuned model file. This will not run the experiment as an Azure job but is sufficient for creating deployment. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\r\n",
        "import pandas as pd \r\n",
        "import numpy as np\r\n",
        "from transformers import AutoTokenizer, TFAutoModel\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "# Parameters\r\n",
        "BATCH_SIZE = 16\r\n",
        "EPOCHS = 1\r\n",
        "CLEAN_TEXT = False\r\n",
        "ADD_DENSE = False\r\n",
        "DENSE_DIM = 64\r\n",
        "ADD_DROPOUT = False\r\n",
        "DROPOUT = .2\r\n",
        "TRAIN_BASE = True\r\n",
        "\r\n",
        "# Data specific fields\r\n",
        "textCol = \"text\"\r\n",
        "clusterCol = \"label\"\r\n",
        "\r\n",
        "\r\n",
        "def bert_encode(data, maximum_len):\r\n",
        "    input_ids = []\r\n",
        "    attention_masks = []\r\n",
        "\r\n",
        "    for iterator in range(len(data.text)):\r\n",
        "        encoded = tokenizer.encode_plus(data.text.iloc[iterator],\r\n",
        "                                        add_special_tokens = True,\r\n",
        "                                        max_length = maximum_len,\r\n",
        "                                        pad_to_max_length = True,\r\n",
        "                                        return_attention_mask = True)\r\n",
        "\r\n",
        "        input_ids.append(encoded['input_ids'])\r\n",
        "        attention_masks.append(encoded['attention_mask'])\r\n",
        "    \r\n",
        "    return np.array(input_ids), np.array(attention_masks)\r\n",
        "\r\n",
        "\r\n",
        "def build_model(model_layer, learning_rate, add_dense = ADD_DENSE,\r\n",
        "                dense_dim = DENSE_DIM, add_dropout = ADD_DROPOUT, dropout = DROPOUT):\r\n",
        "    \r\n",
        "    # define inputs\r\n",
        "    input_ids = tf.keras.Input(shape = (128,), dtype ='int32')\r\n",
        "    attention_masks = tf.keras.Input(shape =(128,), dtype ='int32')\r\n",
        "\r\n",
        "    # insert BERT layer\r\n",
        "    sequence_output = model_layer(input_ids, attention_masks)\r\n",
        "\r\n",
        "    # choose only last hidden state\r\n",
        "    output = sequence_output[0]\r\n",
        "    output = output[:,0,:]\r\n",
        "\r\n",
        "    output = tf.keras.layers.Dense(32, activation = 'relu')(output)\r\n",
        "    output = tf.keras.layers.Dense(7, activation = 'softmax')(output) # changed the last layer to 7 from 5 since we have 7 clusters in our data\r\n",
        "\r\n",
        "    # assemble and compile\r\n",
        "\r\n",
        "    model = tf.keras.models.Model(inputs = [input_ids, attention_masks], outputs = output)\r\n",
        "    model.compile(tf.keras.optimizers.Adam(learning_rate = learning_rate),\r\n",
        "                  loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\r\n",
        "    \r\n",
        "    return model\r\n",
        "\r\n",
        "\r\n",
        "# import data and rename fields - HSBC needs to change this bit to work with their data\r\n",
        "train = pd.read_csv('src/data/train.csv')\r\n",
        "train = train[[textCol, clusterCol]]\r\n",
        "train.rename(columns={textCol: 'text', clusterCol: 'label'}, inplace=True)\r\n",
        "\r\n",
        "# Import models (from folder in your Azure directory)\r\n",
        "pathToHFModels = 'src/deployment/hf_model' # update this path to where you are storing the model\r\n",
        "tokenizer = AutoTokenizer.from_pretrained(pathToHFModels)\r\n",
        "model = TFAutoModel.from_pretrained(pathToHFModels)\r\n",
        "\r\n",
        "# Tokenize\r\n",
        "if TRAIN_BASE:\r\n",
        "    train_inputs_ids, train_attention_masks = bert_encode(train, 128)\r\n",
        "\r\n",
        "# Build the model\r\n",
        "BERT_base = build_model(model, learning_rate = 1e-5)\r\n",
        "\r\n",
        "# Create a checkpoint\r\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('src/deployment/model/hf_model.h5',\r\n",
        "    monitor = 'val_loss', save_best_only = True, save_weights_only = True)\r\n",
        "\r\n",
        "# Train the model\r\n",
        "if TRAIN_BASE:\r\n",
        "    history = BERT_base.fit([train_inputs_ids, train_attention_masks], train.label,\r\n",
        "                            validation_split = .2, epochs = EPOCHS, callbacks = [checkpoint])\r\n",
        "\r\n",
        "# Save the trained model locally\r\n",
        "BERT_base.save('src/deployment/model/custom_bert.h5') # update this path to where you wish to save the model"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1672760376311
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Run model training as an Azure experiment/job***"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import command\r\n",
        "from azure.ai.ml import Input\r\n",
        "import mlflow\r\n",
        "\r\n",
        "job = command(\r\n",
        "    code=\"./src/\",  # location of source code\r\n",
        "    command=\"python jam_model.py\",\r\n",
        "    environment=\"jam_environment:1\",\r\n",
        "    compute=\"dg-compute1\",\r\n",
        "    experiment_name=\"train_model\",\r\n",
        "    display_name=\"jam-job\",\r\n",
        ")\r\n",
        "\r\n",
        "# submit the command\r\n",
        "ml_client.jobs.create_or_update(job)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1672760487856
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Register the model***"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# register model from inline script\r\n",
        "\r\n",
        "from azure.ai.ml.entities import Model\r\n",
        "from azure.ai.ml.constants import AssetTypes\r\n",
        "\r\n",
        "# register the model\r\n",
        "model = Model(\r\n",
        "    path=\"src/deployment\",\r\n",
        "    type=AssetTypes.CUSTOM_MODEL,\r\n",
        "    name=\"deployment_jam\",\r\n",
        "    description=\"Model created with dummy data to test the process\",\r\n",
        ")\r\n",
        "\r\n",
        "ml_client.models.create_or_update(model)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1672761414190
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# register the model from the job\r\n",
        "\r\n",
        "from azure.ai.ml.entities import Model\r\n",
        "\r\n",
        "model = Model(\r\n",
        "    # the script stores the model as \"model\"\r\n",
        "    path=\"azureml://jobs/{}/outputs/artifacts/paths/outputs/model/\".format(\r\n",
        "        \"dynamic_oil_vy2vpwjjds\"\r\n",
        "    ),\r\n",
        "    name=\"run_model_example\",\r\n",
        "    description=\"Model created from run.\",\r\n",
        "    type=\"custom_model\",\r\n",
        ")\r\n",
        "\r\n",
        "registered_model = ml_client.models.create_or_update(model=model)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1671087433777
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Create endpoint for deployment***"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create endpoint\r\n",
        "\r\n",
        "import uuid\r\n",
        "\r\n",
        "# Creating a unique name for the endpoint\r\n",
        "online_endpoint_name = \"endpoint-\" + str(uuid.uuid4())[:8]\r\n",
        "\r\n",
        "from azure.ai.ml.entities import (\r\n",
        "    ManagedOnlineEndpoint,\r\n",
        "    ManagedOnlineDeployment,\r\n",
        "    Model,\r\n",
        "    Environment,\r\n",
        ")\r\n",
        "\r\n",
        "# create an online endpoint\r\n",
        "endpoint = ManagedOnlineEndpoint(\r\n",
        "    name=online_endpoint_name,\r\n",
        "    description=\"\",\r\n",
        "    auth_mode=\"key\",\r\n",
        ")\r\n",
        "\r\n",
        "endpoint = ml_client.begin_create_or_update(endpoint)\r\n",
        "\r\n",
        "endpoint = ml_client.online_endpoints.get(name=online_endpoint_name)\r\n",
        "\r\n",
        "print(\r\n",
        "    f'Endpoint \"{endpoint.name}\" with provisioning state \"{endpoint.provisioning_state}\" is retrieved')\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1672761534071
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Register model to endpoint***"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# deploy model to endpoint\r\n",
        "\r\n",
        "model = 'deployment_jam:1'\r\n",
        "\r\n",
        "from azure.ai.ml.entities import CodeConfiguration\r\n",
        "\r\n",
        "# create an online deployment.\r\n",
        "jam_deployment = ManagedOnlineDeployment(\r\n",
        "    name=\"JAM-deployment\",\r\n",
        "    endpoint_name=online_endpoint_name,\r\n",
        "    model=model,\r\n",
        "    code_configuration=CodeConfiguration(code=\"src\", scoring_script=\"score_realtime.py\"),\r\n",
        "    environment=\"jam_environment:3\",\r\n",
        "    instance_type=\"Standard_DS3_v2\",\r\n",
        "    instance_count=1,\r\n",
        ")\r\n",
        "\r\n",
        "jam_deployment = ml_client.begin_create_or_update(jam_deployment)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1672761723240
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Test deployment***"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\r\n",
        "import pandas as pd\r\n",
        "from azure.ai.ml import MLClient\r\n",
        "\r\n",
        "# predict using the deployed model\r\n",
        "result = ml_client.online_endpoints.invoke(\r\n",
        "    endpoint_name=online_endpoint_name,\r\n",
        "    request_file=\"src/data/test.txt\",\r\n",
        "    deployment_name=\"jam-deployment\",\r\n",
        ")\r\n",
        "\r\n",
        "results = json.loads(result)\r\n",
        "result_data = pd.DataFrame(eval(results))\r\n",
        "result_data"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1672762195279
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}