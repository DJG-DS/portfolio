{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Multi-Agent AI Solution\n",
    "\n",
    "Daniel Godden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document aims to understand the process of building a multi-agent approach to solving generative AI problems. By breaking down the problem into smaller tasks, multi-agent AI can solve more complex problems. The aim of this document is to work through the process of building a solution in Python using AutoGen Studio. This solution will take a PDF file and create an article/blog post. To achieve this, the problem will be broken down into various agents performing tasks such as: PDF extraction, summarisation, keyword extraction, title generation, and blog formatting. The LLM we will be using throughout this project is OpenAIâ€™s GPT-4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Based Multi-Agent Approach\n",
    "\n",
    "### Workflow\n",
    "\n",
    "1. **PDF Extraction**: Extracts text from an uploaded PDF document.\n",
    "2. **Summarisation**: Breaks down the text into smaller chunks and generates summaries using GPT-4.\n",
    "3. **Keyword Extraction**: Extracts keywords and key topics from the text.\n",
    "4. **Title Generation**: Creates a title for the blog post.\n",
    "5. **Blog Formatting**: Combines the title, summary, and keywords into a well-structured blog post.\n",
    "6. **Coordinator**: Agent to coordinate other agents.\n",
    "\n",
    "### Technology Stack\n",
    "\n",
    "- **OpenAI GPT-4**: Used for generating summaries, extracting keywords, and creating titles.\n",
    "- **PyPDF2**: Python library used to extract text from PDF files.\n",
    "- **CrewAI**: Coordinates the multi-agent system, allowing agents to work together efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code demonstrates how to load environment variables from a .env file and retrieve a specific environment variable, in this case, an API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the API key from environment variables\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. PDF Extraction Agent\n",
    "\n",
    "This agent extracts the text from the PDF document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "class PDFExtractionAgent:\n",
    "    \"\"\"\n",
    "    A class for extracting text from a PDF file.\n",
    "    \"\"\"\n",
    "\n",
    "    def extract_text(self, pdf_path):\n",
    "        \"\"\"\n",
    "        Extracts text from the given PDF file.\n",
    "\n",
    "        Args:\n",
    "            pdf_path (str): Path to the PDF file.\n",
    "\n",
    "        Returns:\n",
    "            str: The extracted text from the PDF.\n",
    "\n",
    "        Example:\n",
    "            agent = PDFExtractionAgent()\n",
    "            text = agent.extract_text('example.pdf')\n",
    "        \"\"\"\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text()\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Summarisation Agent\n",
    "\n",
    "This agent splits the extracted text into smaller chunks and generates a summary for each chuck using GPT-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Set up OpenAI API key\n",
    "openai.api_key = api_key\n",
    "\n",
    "TOKEN_LIMIT = 2000  # Set token limit to avoid rate limits\n",
    "\n",
    "def split_text(text, max_tokens):\n",
    "    \"\"\"\n",
    "    Splits the input text into smaller chunks based on a token limit.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to split.\n",
    "        max_tokens (int): The maximum number of tokens per chunk.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of text chunks that fit within the token limit.\n",
    "\n",
    "    Example:\n",
    "        chunks = split_text(text, 2000)\n",
    "    \"\"\"\n",
    "    sentences = text.split('.')\n",
    "    chunks = []\n",
    "    current_chunk = ''\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk.split()) + len(sentence.split()) <= max_tokens:\n",
    "            current_chunk += sentence + '.'\n",
    "        else:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = sentence + '.'\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "class SummarisationAgent:\n",
    "    \"\"\"\n",
    "    A class to generate text summaries using the OpenAI GPT model.\n",
    "    \"\"\"\n",
    "\n",
    "    def generate_summary(self, text, chunk_size=TOKEN_LIMIT):\n",
    "        \"\"\"\n",
    "        Generates a summary of the input text by splitting it into chunks.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to summarize.\n",
    "            chunk_size (int): Maximum number of tokens per chunk (default is TOKEN_LIMIT).\n",
    "\n",
    "        Returns:\n",
    "            str: The combined summary of the input text.\n",
    "\n",
    "        Example:\n",
    "            agent = SummarisationAgent()\n",
    "            summary = agent.generate_summary(text)\n",
    "        \"\"\"\n",
    "        text_chunks = split_text(text, chunk_size)\n",
    "        summary_chunks = []\n",
    "\n",
    "        for chunk in text_chunks:\n",
    "            prompt = f\"Summarize the following text for a blog post:\\n\\n{chunk}\"\n",
    "            response = openai.chat.completions.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.7,\n",
    "                max_tokens=2000,\n",
    "            )\n",
    "            summary = response.choices[0].message.content\n",
    "            summary_chunks.append(summary)\n",
    "        \n",
    "        final_summary = ' '.join(summary_chunks)\n",
    "        \n",
    "        return final_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Keyword Agent\n",
    "\n",
    "This agent extracts keywords from the text using the GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeywordAgent:\n",
    "    \"\"\"\n",
    "    A class to extract keywords from a given text using the OpenAI GPT model.\n",
    "    \"\"\"\n",
    "\n",
    "    def extract_keywords(self, text, chunk_size=TOKEN_LIMIT):\n",
    "        \"\"\"\n",
    "        Extracts keywords from the provided text by splitting it into smaller chunks.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text from which to extract keywords.\n",
    "            chunk_size (int): The maximum number of tokens allowed per chunk (default is TOKEN_LIMIT).\n",
    "\n",
    "        Returns:\n",
    "            str: A string of extracted keywords, separated by commas.\n",
    "\n",
    "        Example:\n",
    "            agent = KeywordAgent()\n",
    "            keywords = agent.extract_keywords(text)\n",
    "        \"\"\"\n",
    "        text_chunks = split_text(text, chunk_size)\n",
    "        keyword_chunks = []\n",
    "\n",
    "        for chunk in text_chunks:\n",
    "            prompt = f\"Extract keywords from the following text:\\n\\n{chunk}\"\n",
    "            response = openai.chat.completions.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.5,\n",
    "                max_tokens=200,\n",
    "            )\n",
    "            keywords = response.choices[0].message.content\n",
    "            keyword_chunks.append(keywords)\n",
    "        \n",
    "        final_keywords = ', '.join(keyword_chunks)\n",
    "        return final_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Title Agent\n",
    "\n",
    "This agent generates title for blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitleAgent:\n",
    "    \"\"\"\n",
    "    A class to generate an engaging title for a blog post using the OpenAI GPT model.\n",
    "    \"\"\"\n",
    "\n",
    "    def generate_title(self, text, chunk_size=TOKEN_LIMIT):\n",
    "        \"\"\"\n",
    "        Generates a title for the provided text by using the first chunk of the content.\n",
    "\n",
    "        Args:\n",
    "            text (str): The content based on which the title is generated.\n",
    "            chunk_size (int): The maximum number of tokens in the first chunk (default is TOKEN_LIMIT).\n",
    "\n",
    "        Returns:\n",
    "            str: A generated title for the blog post.\n",
    "\n",
    "        Example:\n",
    "            agent = TitleAgent()\n",
    "            title = agent.generate_title(text)\n",
    "        \"\"\"\n",
    "        # Use only the first chunk for title generation\n",
    "        text_chunk = split_text(text, chunk_size)[0]\n",
    "        prompt = f\"Generate an engaging title for a blog post based on this content:\\n\\n{text_chunk}\"\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=60,\n",
    "        )\n",
    "        title = response.choices[0].message.content\n",
    "        return title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Blog Formatting Agent\n",
    "\n",
    "This agent combines the tile, summary, and keywords into a fromatted blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FormattingAgent:\n",
    "    \"\"\"\n",
    "    A class to format a blog post by combining a title, summary, and keywords.\n",
    "    \"\"\"\n",
    "\n",
    "    def format_blog_post(self, title, summary, keywords):\n",
    "        \"\"\"\n",
    "        Formats a blog post with a given title, summary, and keywords.\n",
    "\n",
    "        Args:\n",
    "            title (str): The title of the blog post.\n",
    "            summary (str): The summary or main content of the blog post.\n",
    "            keywords (str): A string of keywords related to the blog post.\n",
    "\n",
    "        Returns:\n",
    "            str: A formatted blog post with the title, summary, and keywords.\n",
    "\n",
    "        Example:\n",
    "            agent = FormattingAgent()\n",
    "            blog_post = agent.format_blog_post(\"Blog Title\", \"This is a summary.\", \"keyword1, keyword2\")\n",
    "        \"\"\"\n",
    "        blog_post = f\"# {title}\\n\\n{summary}\\n\\n**Keywords**: {keywords}\\n\"\n",
    "        return blog_post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Coordinator\n",
    "\n",
    "The coordinator class manages all the agents, orchestrating the workflow from PDF extraction to blog post generation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlogPostGeneratorCoordinator:\n",
    "    \"\"\"\n",
    "    A class to coordinate the generation of a complete blog post from a PDF file \n",
    "    by leveraging multiple agents (PDF extraction, summarisation, keyword extraction, title generation, and formatting).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the BlogPostGeneratorCoordinator by creating instances of the required agents:\n",
    "        - PDFExtractionAgent for extracting text from PDF.\n",
    "        - SummarisationAgent for summarising the extracted text.\n",
    "        - KeywordAgent for extracting keywords.\n",
    "        - TitleAgent for generating a title.\n",
    "        - FormattingAgent for formatting the blog post.\n",
    "        \"\"\"\n",
    "        self.pdf_agent = PDFExtractionAgent()\n",
    "        self.summary_agent = SummarisationAgent()\n",
    "        self.keyword_agent = KeywordAgent()\n",
    "        self.title_agent = TitleAgent()\n",
    "        self.formatting_agent = FormattingAgent()\n",
    "\n",
    "    def generate_blog_post_from_pdf(self, pdf_path):\n",
    "        \"\"\"\n",
    "        Generates a complete blog post from a PDF file by coordinating the use of several agents.\n",
    "\n",
    "        Args:\n",
    "            pdf_path (str): The file path to the PDF document.\n",
    "\n",
    "        Returns:\n",
    "            str: A formatted blog post containing a title, summary, and keywords, based on the content of the PDF.\n",
    "\n",
    "        Example:\n",
    "            coordinator = BlogPostGeneratorCoordinator()\n",
    "            blog_post = coordinator.generate_blog_post_from_pdf('document.pdf')\n",
    "        \"\"\"\n",
    "        # Agent 1: Extract text from PDF\n",
    "        pdf_text = self.pdf_agent.extract_text(pdf_path)\n",
    "\n",
    "        # Agent 2: Generate a summary for the blog post\n",
    "        summary = self.summary_agent.generate_summary(pdf_text)\n",
    "\n",
    "        # Agent 3: Extract keywords\n",
    "        keywords = self.keyword_agent.extract_keywords(pdf_text)\n",
    "\n",
    "        # Agent 4: Generate a title for the blog post\n",
    "        title = self.title_agent.generate_title(pdf_text)\n",
    "\n",
    "        # Agent 5: Format the blog post\n",
    "        blog_post = self.formatting_agent.format_blog_post(title, summary, keywords)\n",
    "\n",
    "        return blog_post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the agents have been configured, the process can be invoked. This will create a text file and word document file containing the blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the path to the PDF file that will be used for generating the blog post\n",
    "    pdf_path = \"C:/Users/DanielGodden/Downloads/Generative-AI-and-LLMs-for-Dummies.pdf\"\n",
    "\n",
    "    # Create an instance of BlogPostGeneratorCoordinator to handle the blog post generation\n",
    "    coordinator = BlogPostGeneratorCoordinator()\n",
    "\n",
    "    # Generate the blog post by extracting content from the specified PDF\n",
    "    blog_post = coordinator.generate_blog_post_from_pdf(pdf_path)\n",
    "\n",
    "    # Remove the \"Keywords\" section from the blog post before saving to a text file\n",
    "    blog_post_without_keywords = blog_post.split(\"**Keywords**\")[0]  # Split and exclude keywords section\n",
    "\n",
    "    # Save the blog post (without keywords) to a text file\n",
    "    with open(\"generated_blog_post.txt\", \"w\") as text_file:\n",
    "        text_file.write(blog_post_without_keywords)  # Write blog post content to the file\n",
    "        print(blog_post_without_keywords)  # Optionally print the blog post to the console\n",
    "\n",
    "    # Create a new Word document object to save the blog post in DOCX format\n",
    "    doc = Document()\n",
    "\n",
    "    # Split the blog post content into lines for further processing\n",
    "    lines = blog_post_without_keywords.split('\\n')\n",
    "\n",
    "    # Extract and format the title from the first line of the blog post\n",
    "    title = lines[0].strip(\"# \").strip()  # Remove the markdown-style \"#\" and surrounding spaces\n",
    "\n",
    "    # Add the extracted title as a heading to the Word document\n",
    "    doc.add_heading(title, 0)\n",
    "\n",
    "    # Add the entire blog post content (excluding keywords) as a paragraph in the Word document\n",
    "    doc.add_paragraph(blog_post_without_keywords)\n",
    "\n",
    "    # Save the blog post as a Word document with the specified filename\n",
    "    doc.save(\"blog_post.docx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up and Running Autogen Studio\n",
    "\n",
    "### Introduction to Autogen Studio\n",
    "\n",
    "Autogen Studio is a powerful tool designed to simplify the creation and management of AI-driven agents. These agents can automate tasks, interact with users, or perform various operations based on predefined instructions or AI models like GPT. Autogen Studio provides an intuitive user interface that allows you to design, test, and deploy these agents with ease.\n",
    "\n",
    "In this guide, we'll walk you through the steps to set up Autogen Studio in your development environment and get it running.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Before you begin, ensure that you have Python installed on your machine. This guide assumes you are using an IDE (Integrated Development Environment) like Visual Studio Code.\n",
    "\n",
    "### Step-by-Step Setup Guide\n",
    "\n",
    "#### 1. Create a New Project Folder\n",
    "\n",
    "1. Create a new folder where you want to store your project files.\n",
    "2. Open this folder in your IDE.\n",
    "\n",
    "#### 2. Set Up a Virtual Environment\n",
    "\n",
    "1. Open the terminal in your IDE.\n",
    "2. Create a virtual environment by typing the following command:\n",
    "    ```bash\n",
    "    python -m venv .venv\n",
    "    ```\n",
    "3. Activate the virtual environment:\n",
    "    - On Windows, use:\n",
    "      ```bash\n",
    "      .venv\\Scripts\\activate\n",
    "#### 3. Install Autogen Studio\n",
    "\n",
    "1. With your virtual environment activated, install Autogen Studio using pip:\n",
    "    ```bash\n",
    "    pip install autogenstudio\n",
    "    ```\n",
    "#### 4. Clear the Terminal Screen (Optional)\n",
    "\n",
    "- To keep your terminal clean, you can clear the screen:\n",
    "    - On Windows:\n",
    "      ```bash\n",
    "      cls\n",
    "      ```\n",
    "#### 5. Set Your OpenAI API Key\n",
    "\n",
    "1. Visit the OpenAI API Keys page: [OpenAI API Keys](https://platform.openai.com/api-keys).\n",
    "2. Create a new secret key.\n",
    "3. Set the API key in your terminal:\n",
    "    ```bash\n",
    "    set OPEN_API_KEY=\"your-secret-key\"\n",
    "    ```\n",
    "   Replace `your-secret-key` with the actual key you generated.\n",
    "\n",
    "#### 6. Start Autogen Studio\n",
    "\n",
    "1. Start the Autogen Studio UI by typing the following command in your terminal:\n",
    "    ```bash\n",
    "    autogenstudio ui\n",
    "    ```\n",
    "2. The terminal will display a message similar to:\n",
    "    ```\n",
    "    Uvicorn running on http://127.0.0.1:8081\n",
    "    ```\n",
    "3. Hold `CTRL` and click on the provided address (`http://127.0.0.1:8081`). This will open the Autogen Studio UI in your default web browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autogen Studio UI Overview\n",
    "\n",
    "Autogen Studio is a powerful platform designed to streamline the creation and management of automated workflows using AI-driven tools. The interface is user-friendly, making it accessible for both beginners and advanced users. The UI is divided into two main sections: **Build** and **Playground**.\n",
    "\n",
    "### Build Section\n",
    "\n",
    "The **Build** section is where you design, configure, and manage the components that drive your automated processes. It is divided into four main subsections:\n",
    "\n",
    "#### 1. Skills\n",
    "In the **Skills** section, you define the specific abilities your AI models will use. Skills can range from natural language processing to data manipulation, and they are the building blocks for your workflows. Here, you can create, edit, and manage these skills, tailoring them to suit the needs of your projects.\n",
    "\n",
    "#### 2. Models\n",
    "The **Models** section is where you select and configure the AI models that power your skills. Autogen Studio supports a variety of models, allowing you to choose the most appropriate one based on your task requirements. You can also manage model versions, update configurations, and monitor performance metrics.\n",
    "\n",
    "#### 3. Agents\n",
    "In the **Agents** section, you design and manage the agents that will execute tasks using the skills and models you've set up. Agents are essentially the active entities that perform actions based on the input they receive. You can customize their behavior, set triggers, and define how they interact with other agents or external systems.\n",
    "\n",
    "#### 4. Workflows\n",
    "The **Workflows** section ties everything together. Here, you create automated processes by linking skills, models, and agents into cohesive workflows. Workflows are visualized as flowcharts, making it easy to understand the sequence of operations. You can drag and drop components, set conditions, and define branching logic to create complex automation tasks.\n",
    "\n",
    "### Playground Section\n",
    "\n",
    "The **Playground** section is your testing ground. Once you've built your workflows, you can simulate them in the Playground to see how they perform in real-time. This section allows you to tweak parameters, observe outputs, and make adjustments before deploying your workflows in a live environment.\n",
    "\n",
    "### Setting Up Your Environment\n",
    "\n",
    "#### Step 1: Define Your Skills\n",
    "Start by navigating to the **Skills** section in the Build tab. Here, you'll define the core capabilities your agents will need. For example, if you're building a chatbot, you might create skills for understanding user queries, retrieving information, and generating responses. In the case of the above problem, the PDF extraction and summarisation agents code should be adapted.\n",
    "\n",
    "#### Step 2: Choose and Configure Models\n",
    "Next, move to the **Models** section. Select an appropriate model for each skill you've defined. Configure the models according to your specific needs, adjusting parameters such as input data types, output formats, and processing speed.\n",
    "\n",
    "#### Step 3: Design Your Agents\n",
    "In the **Agents** section, create agents that will use the skills and models you've set up. Define their roles, set triggers for when they should act, and determine how they should interact with other agents or external systems. You can define them using the same configuration as given above, using the prompt as the message you provide them.\n",
    "\n",
    "#### Step 4: Build Workflows\n",
    "With your agents in place, head to the **Workflows** section. Start by dragging and dropping skills, models, and agents onto the workflow canvas. Connect these components to form a logical sequence of operations. Add conditions, loops, and branches as needed to handle different scenarios.\n",
    "\n",
    "#### Step 5: Test in the Playground\n",
    "Before deploying your workflow, test it in the **Playground** section. Run simulations to ensure everything functions as expected. Use this opportunity to fine-tune your setup, making any necessary adjustments to improve performance or accuracy.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Autogen Studio's intuitive interface and modular design make it easy to build and deploy sophisticated automation workflows. By following the structured process of defining skills, configuring models, designing agents, and constructing workflows, you can create powerful AI-driven solutions tailored to your specific needs. The Playground section offers a safe environment to test and refine your workflows, ensuring they are ready for real-world application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
