{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Multi PDF RAG Chatbot Using Langchain and Streamlit\n",
    "\n",
    "Daniel Godden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "\n",
    "This project walks through the steps required to build a Multi-PDF Retrieval-Augmented Generation (RAG) Streamlit-based web application that enables users to read, process, and interact with PDF data through a conversational AI chatbot. The project was initially developed by Paras Madan and the original guide can be found [here](https://medium.com/gopenai/building-a-multi-pdf-rag-chatbot-langchain-streamlit-with-code-d21d0a1cf9e5).\n",
    "\n",
    "### Streamlit: A Framework for Data Science Web Applications\n",
    "\n",
    "Streamlit is an open-source Python framework specifically designed for creating custom web applications tailored to data science and machine learning projects. Its user-friendly interface allows data scientists and machine learning engineers to rapidly develop interactive, visually appealing, and shareable web apps directly from Python scripts. With Streamlit, developers can build complex applications without requiring extensive web development expertise, making it a powerful tool for bridging the gap between data analysis and user interaction.\n",
    "\n",
    "### LangChain: Advanced Language Model Integration\n",
    "\n",
    "LangChain is a versatile framework that empowers developers to build sophisticated applications leveraging large language models (LLMs). It excels in creating structured workflows that integrate external data sources and maintain context across interactions, making it ideal for developing conversational agents and other advanced AI-driven applications. LangChain allows developers to chain together multiple steps or tasks, facilitating complex use cases such as conversational AI, data-driven content generation, and more. With support for custom tools and flexible pipelines, LangChain enhances the functionality of language models, enabling the development of powerful, context-aware applications.\n",
    "\n",
    "### Project Goals\n",
    "\n",
    "The main objective of this project is to create a web application that allows users to upload multiple PDF documents, ask questions related to the content of these documents, and receive contextually relevant answers generated by a conversational AI chatbot. This is achieved through the following steps:\n",
    "\n",
    "1. **PDF Text Extraction**: Using PyPDF2, the application extracts text from the uploaded PDF files.\n",
    "\n",
    "2. **Text Chunking**: The extracted text is split into smaller chunks using LangChain's text-splitting tools, making it manageable for processing by the language model.\n",
    "\n",
    "3. **Vector Store Creation**: The text chunks are converted into vector embeddings using Spacy, and these vectors are stored in a FAISS index, which allows for efficient similarity search and information retrieval.\n",
    "\n",
    "4. **Conversational Chain Setup**: A conversational chain is established using LangChain, where the language model (OpenAI's GPT) interacts with the user, processing their queries and retrieving relevant information from the vector store.\n",
    "\n",
    "5. **Streamlit Integration**: The entire process is wrapped into a user-friendly web application using Streamlit, providing an intuitive interface for users to upload PDFs, ask questions, and view the AI-generated responses.\n",
    "\n",
    "### Why This Project Matters\n",
    "\n",
    "With the increasing reliance on unstructured data, particularly in the form of PDFs, this project demonstrates how modern AI and web development tools can be harnessed to unlock the potential of such data. By integrating advanced language models with efficient retrieval mechanisms, the project offers a practical solution for interacting with and extracting value from complex documents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pip Installs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code cell uses the `%pip install` command to install a set of Python libraries necessary for the notebook's operations. Below is a detailed explanation of each library being installed:\n",
    "\n",
    "1. **streamlit**: \n",
    "   - A framework for creating interactive web applications directly from Python scripts. It is particularly useful for data science and machine learning projects, allowing you to quickly build and deploy data apps.\n",
    "\n",
    "2. **PyPDF2**: \n",
    "   - A library used for working with PDF files in Python. It supports tasks such as extracting text, merging PDFs, and more.\n",
    "\n",
    "3. **python-dotenv**: \n",
    "   - This library is used to load environment variables from a `.env` file into the environment. It's useful for managing secret keys and configuration settings without hardcoding them in the script.\n",
    "\n",
    "4. **Langchain**: \n",
    "   - A framework designed to simplify the development of applications powered by large language models. It provides tools for chaining together different components like prompts, memory, and agents.\n",
    "\n",
    "5. **langchain_community**: \n",
    "   - A collection of community-driven tools, extensions, and integrations for the Langchain framework.\n",
    "\n",
    "6. **langchain_anthropic**: \n",
    "   - A specific extension of Langchain designed to work with models provided by Anthropic, a company known for developing advanced AI models.\n",
    "\n",
    "7. **langchain_openai**: \n",
    "   - Another extension of Langchain tailored for integrating with OpenAI's language models, making it easier to build applications that utilize OpenAI's capabilities.\n",
    "\n",
    "8. **Spacy**: \n",
    "   - An advanced Natural Language Processing (NLP) library in Python, designed for production use. It offers pre-trained models, tokenization, part-of-speech tagging, named entity recognition, and more.\n",
    "\n",
    "9. **faiss-cpu**: \n",
    "   - A library developed by Facebook AI Research (FAISS) for efficient similarity search and clustering of dense vectors, commonly used in tasks involving large datasets and machine learning.\n",
    "\n",
    "This command will download and install the latest versions of these libraries, ensuring that all dependencies are met.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install streamlit PyPDF2 python-dotenv Langchain langchain_community langchain_anthropic langchain_openai Spacy faiss-cpu "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code cell performs several critical setup tasks:\n",
    "\n",
    "1. **Library Imports**:\n",
    "   - **streamlit as st**: Imports the Streamlit library, which is used to create web applications directly from Python.\n",
    "   - **PyPDF2's PdfReader**: A class from the PyPDF2 library used to read PDF files.\n",
    "   - **langchain.text_splitter's RecursiveCharacterTextSplitter**: This is used to split large texts into smaller, manageable chunks, typically for processing with language models.\n",
    "   - **langchain_core.prompts' ChatPromptTemplate**: A tool for defining and managing chat prompts for language models.\n",
    "   - **langchain_community.embeddings' SpacyEmbeddings**: Provides a method to create embeddings (numerical representations) of text using Spacy, an NLP library.\n",
    "   - **langchain_community.vectorstores' FAISS**: FAISS is a library for efficient similarity search and clustering of dense vectors, integrated here for use with Langchain.\n",
    "   - **langchain.tools.retriever's create_retriever_tool**: A utility for creating a retriever tool, which is typically used for fetching relevant pieces of information based on a query.\n",
    "   - **dotenv's load_dotenv**: This function loads environment variables from a `.env` file, which is often used to manage sensitive information like API keys securely.\n",
    "   - **langchain_anthropic's ChatAnthropic**: A class to interact with Anthropic's language models.\n",
    "   - **langchain_openai's ChatOpenAI and OpenAIEmbeddings**: These are used to interact with OpenAI's models and to generate embeddings using OpenAI's tools.\n",
    "   - **os**: Python's built-in library to interact with the operating system. It is used here for setting environment variables and accessing the system's environment.\n",
    "\n",
    "   Note: The line importing the `pipeline` function from `transformers` is commented out, indicating it might be an optional or alternative functionality.\n",
    "\n",
    "2. **Environment Configuration**:\n",
    "   - The line `os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"` is included to prevent errors related to loading duplicate shared libraries, particularly in environments where Intel's Math Kernel Library (MKL) might cause conflicts due to multiple instances of the same library being loaded.\n",
    "\n",
    "3. **Loading Environment Variables**:\n",
    "   - `load_dotenv()` is called to load any environment variables defined in a `.env` file. This is typically where sensitive information like API keys is stored.\n",
    "   - The `api_key` variable retrieves the OpenAI API key from the environment variables, which will be used to authenticate requests to the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.embeddings.spacy_embeddings import SpacyEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from dotenv import load_dotenv\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "#from transformers import pipeline\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "\n",
    "import os\n",
    "\n",
    "'''\n",
    "The line `os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"` is used in Python to prevent errors related to loading duplicate shared libraries, \n",
    "particularly when using the Intel Math Kernel Library (MKL) in multi-threaded environments. \n",
    "It allows the program to continue running by permitting multiple instances of the same library to be loaded without causing conflicts.\n",
    "'''\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "# Load the environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the API key from environment variables\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Processing PDF Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code cell defines two important functions: `pdf_read` and `get_chunks`. These functions are used to extract text from PDF documents and split that text into smaller chunks for further processing.\n",
    "\n",
    "#### `pdf_read(pdf_doc)`\n",
    "- **Purpose**: \n",
    "  - This function extracts and concatenates text from a list of PDF files.\n",
    "- **Arguments**:\n",
    "  - `pdf_doc (list)`: A list of PDF file paths or file-like objects from which text will be extracted.\n",
    "- **Returns**:\n",
    "  - `str`: A single string containing the concatenated text extracted from all pages of the provided PDFs.\n",
    "- **How it works**:\n",
    "  - It initializes a `PdfReader` object for each PDF file in the list.\n",
    "  - It then loops through each page of the PDF, extracting the text and appending it to a cumulative `text` variable.\n",
    "  - The function finally returns the complete text extracted from all the PDFs.\n",
    "\n",
    "#### `get_chunks(text)`\n",
    "- **Purpose**:\n",
    "  - This function splits a large block of text into smaller, manageable chunks that are ideal for processing with language models or other text analysis tools.\n",
    "- **Arguments**:\n",
    "  - `text (str)`: The full text string that needs to be split into chunks.\n",
    "- **Returns**:\n",
    "  - `list`: A list of text chunks, where each chunk is of a specified size, with some overlap between consecutive chunks.\n",
    "- **How it works**:\n",
    "  - It creates an instance of `RecursiveCharacterTextSplitter` with a chunk size of 1000 characters and an overlap of 200 characters.\n",
    "  - The `split_text` method is called to divide the text into these chunks, which ensures that important contextual information is preserved across chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_read(pdf_doc):\n",
    "    \"\"\"\n",
    "    Extracts text from a list of PDF documents.\n",
    "\n",
    "    Args:\n",
    "        pdf_doc (list): A list of PDF file paths or file-like objects.\n",
    "\n",
    "    Returns:\n",
    "        str: The concatenated text extracted from all the pages of the provided PDFs.\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    for pdf in pdf_doc:\n",
    "        pdf_reader = PdfReader(pdf)  # Initialize a PdfReader object for each PDF\n",
    "        for page in pdf_reader.pages:  # Loop through all the pages of the PDF\n",
    "            text += page.extract_text()  # Extract and append text from each page\n",
    "    return text\n",
    "\n",
    "def get_chunks(text):\n",
    "    \"\"\"\n",
    "    Splits a large block of text into smaller chunks.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to be split into chunks.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of text chunks, each of a specified size with some overlap.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    chunks = text_splitter.split_text(text)  # Split the text into chunks with overlap\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Searchable Text Database and Making Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code cell uses a shell command to download a specific pre-trained language model for Spacy:\n",
    "\n",
    "- **Command**: `!python -m spacy download en_core_web_sm`\n",
    "  - This command runs Spacy's model downloader to fetch the **`en_core_web_sm`** model, which is a small, efficient English language model.\n",
    "  - **`en_core_web_sm`**:\n",
    "    - This is one of Spacy's pre-trained models that includes a vocabulary, syntax, and entities suitable for various natural language processing tasks like tokenization, part-of-speech tagging, and named entity recognition.\n",
    "    - It's called \"small\" because it is designed to be lightweight and fast, with a smaller file size, making it suitable for applications where resources are limited or where quick processing is required.\n",
    "  \n",
    "This model is necessary for tasks that involve text processing, such as creating embeddings, parsing text, or recognizing named entities in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code cell performs two main tasks: initializing the Spacy embeddings model and defining a function to create and save a FAISS vector store from text chunks.\n",
    "\n",
    "#### Initializing Spacy Embeddings\n",
    "- **SpacyEmbeddings Initialization**:\n",
    "  - `embeddings = SpacyEmbeddings(model_name=\"en_core_web_sm\")`\n",
    "  - This line initializes an instance of `SpacyEmbeddings` using the `en_core_web_sm` model, which is a small, efficient English language model. This model will be used to generate vector embeddings for text data.\n",
    "\n",
    "#### `vector_store(text_chunks)`\n",
    "- **Purpose**:\n",
    "  - This function creates a FAISS vector store from a list of text chunks and saves it locally. A FAISS vector store allows for efficient similarity search and clustering of these text embeddings, which is useful for tasks like document retrieval or semantic search.\n",
    "\n",
    "- **Arguments**:\n",
    "  - `text_chunks (list of str)`: A list of text strings, typically the output from the `get_chunks` function, that will be converted into vector embeddings.\n",
    "\n",
    "- **Function Process**:\n",
    "  1. **Convert Text Chunks to Vectors**:\n",
    "     - The function uses the initialized `SpacyEmbeddings` model to convert the text chunks into vector embeddings.\n",
    "     - `FAISS.from_texts(text_chunks, embedding=embeddings)` creates a FAISS index from these embeddings, allowing for efficient similarity search later on.\n",
    "  \n",
    "  2. **Save FAISS Vector Store**:\n",
    "     - The created FAISS index is saved to a local file named `\"faiss_db\"` using the `save_local` method. This allows the vector store to be loaded later for retrieval tasks without needing to recompute the embeddings.\n",
    "\n",
    "- **Returns**:\n",
    "  - This function does not return any values; it simply creates and saves the vector store to a file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a SpacyEmbeddings instance with the specified model\n",
    "embeddings = SpacyEmbeddings(model_name=\"en_core_web_sm\")\n",
    "\n",
    "def vector_store(text_chunks):\n",
    "    \"\"\"\n",
    "    Creates and saves a FAISS vector store from a list of text chunks.\n",
    "\n",
    "    This function converts a list of text chunks into vector embeddings using the\n",
    "    Spacy embeddings model, then creates a FAISS index to store these vectors\n",
    "    and saves the index to a local file named \"faiss_db\".\n",
    "\n",
    "    Args:\n",
    "        text_chunks (list of str): A list of text strings to be converted into vectors.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Create a FAISS vector store from the text chunks using the specified embeddings\n",
    "    vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)\n",
    "    \n",
    "    # Save the created FAISS index to a local file\n",
    "    vector_store.save_local(\"faiss_db\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Conversational AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code cell defines two important functions: `get_conversational_chain` and `user_input`. These functions are responsible for setting up a conversational agent, retrieving responses to user queries, and managing the interaction between the user and the AI model.\n",
    "\n",
    "#### `get_conversational_chain(tools, ques)`\n",
    "- **Purpose**:\n",
    "  - This function initializes a conversational chain using a language model and specified tools, processes a user query, and generates a response.\n",
    "- **Arguments**:\n",
    "  - `tools (list)`: A list of tools that the conversational agent can use to retrieve or process information.\n",
    "  - `ques (str)`: The user's question or input that needs to be answered.\n",
    "- **Function Process**:\n",
    "  1. **Language Model Initialization**:\n",
    "     - The function initializes a language model (`ChatOpenAI`) with specific settings, including the model name (`gpt-3.5-turbo`), a temperature setting (which controls the randomness of the model's output), and the API key for authentication.\n",
    "  \n",
    "  2. **Prompt Template Creation**:\n",
    "     - A prompt template is defined using `ChatPromptTemplate.from_messages`. This template instructs the language model to answer questions based on the provided context and ensures that if the answer is not available, the model will state so without providing incorrect information.\n",
    "  \n",
    "  3. **Tool and Agent Creation**:\n",
    "     - The tools are wrapped in a list, and an agent is created using `create_tool_calling_agent`, which allows the agent to use both the language model and the specified tools to generate responses.\n",
    "  \n",
    "  4. **Agent Execution**:\n",
    "     - The `AgentExecutor` is initialized with the agent, tools, and verbosity settings. It processes the input question and generates a response.\n",
    "  \n",
    "  5. **Response Handling**:\n",
    "     - The generated response is printed to the console and displayed on the Streamlit interface using `st.write`.\n",
    "\n",
    "- **Returns**:\n",
    "  - This function does not return a value but instead prints and displays the response.\n",
    "\n",
    "#### `user_input(user_question)`\n",
    "- **Purpose**:\n",
    "  - This function manages user input by loading a local FAISS database, creating a retriever tool, and passing the user's question to the conversational chain for processing.\n",
    "- **Arguments**:\n",
    "  - `user_question (str)`: The question asked by the user that needs to be answered by the system.\n",
    "- **Function Process**:\n",
    "  1. **Loading FAISS Index**:\n",
    "     - The function loads a previously saved FAISS index (a vector store) from a local file using `FAISS.load_local`. This index contains the vector embeddings generated from the text chunks.\n",
    "  \n",
    "  2. **Creating a Retriever Tool**:\n",
    "     - A retriever tool is created from the loaded FAISS index using `new_db.as_retriever()`. This tool is configured with a specific name and description that clarify its purpose.\n",
    "  \n",
    "  3. **Processing User Input**:\n",
    "     - The user's question is passed to the `get_conversational_chain` function, which uses the retriever tool and the language model to generate and display a response.\n",
    "  \n",
    "- **Returns**:\n",
    "  - This function does not return a value but processes the user question and displays the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conversational_chain(tools, ques):\n",
    "    \"\"\"\n",
    "    Initializes a conversational chain using a language model and tools, and retrieves a response for a given question.\n",
    "\n",
    "    This function sets up a conversational agent using the specified language model and tools, then \n",
    "    uses the agent to process a question and print the response. It also writes the response to the Streamlit interface.\n",
    "\n",
    "    Args:\n",
    "        tools (list): A list of tools to be used by the conversational agent.\n",
    "        ques (str): The question or input for which the response is to be generated.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Initialize the language model with specified settings\n",
    "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, api_key=api_key)\n",
    "    # Use Hugging Face's transformers pipeline for text generation\n",
    "    #generator = pipeline('text-generation', model='gpt2')\n",
    "    \n",
    "    # Generate a response using the Hugging Face model\n",
    "    #response = generator(ques, max_length=100, num_return_sequences=1)[0]['generated_text']\n",
    "    \n",
    "\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"You are a helpful assistant. Answer the question as detailed as possible from the provided context, make sure to provide all the details, if the answer is not in\n",
    "                provided context just say, \"answer is not available in the context\", don't provide the wrong answer\"\"\",\n",
    "            ),\n",
    "            (\"placeholder\", \"{chat_history}\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "            (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Wrap the tools into a list\n",
    "    tool = [tools]\n",
    "    \n",
    "    # Create an agent that uses the language model and tools to process inputs\n",
    "    agent = create_tool_calling_agent(llm, tool, prompt)\n",
    "    \n",
    "    # Initialize the agent executor with the agent, tools, and verbosity setting\n",
    "    agent_executor = AgentExecutor(agent=agent, tools=tool, verbose=True)\n",
    "    \n",
    "    # Invoke the agent executor with the given question and get the response\n",
    "    response = agent_executor.invoke({\"input\": ques})\n",
    "    \n",
    "    # Print the response to the console\n",
    "    print(response)\n",
    "    \n",
    "    # Display the response in the Streamlit interface\n",
    "    st.write(\"Reply: \", response)#['output'])\n",
    "\n",
    "def user_input(user_question):\n",
    "    \"\"\"\n",
    "    Handles user input by loading a local FAISS database, creating a retriever tool, \n",
    "    and using it to answer the user's question through a conversational chain.\n",
    "\n",
    "    This function loads a previously saved FAISS index, sets up a retriever tool for extracting information \n",
    "    from the database, and then passes the user's question to the conversational chain for a response.\n",
    "\n",
    "    Args:\n",
    "        user_question (str): The question asked by the user for which an answer is needed.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Load the FAISS index from a local file\n",
    "    new_db = FAISS.load_local(\"faiss_db\", embeddings, allow_dangerous_deserialization=True)\n",
    "    \n",
    "    # Create a retriever tool from the loaded FAISS index\n",
    "    retriever = new_db.as_retriever()\n",
    "    \n",
    "    # Set up the retriever tool with a specific name and description\n",
    "    retrieval_chain = create_retriever_tool(retriever, \"pdf_extractor\", \"This tool is to give answer to queries from the pdf\")\n",
    "    \n",
    "    # Use the conversational chain to handle the user question\n",
    "    get_conversational_chain(retrieval_chain, user_question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code cell defines the `main` function, which sets up and runs a Streamlit application for interacting with PDF files through a question-answering interface. The app allows users to upload PDF files, ask questions based on the content of these files, and receive answers generated by a language model.\n",
    "\n",
    "#### `main()`\n",
    "- **Purpose**:\n",
    "  - The `main` function initializes and configures the Streamlit app, handling user inputs, file uploads, and PDF processing.\n",
    "\n",
    "- **Function Process**:\n",
    "  1. **Streamlit Page Configuration**:\n",
    "     - `st.set_page_config(page_title=\"Chat PDF\")`: This sets the configuration for the Streamlit app, specifically the title of the web page.\n",
    "  \n",
    "  2. **Header Setup**:\n",
    "     - `st.header(\"RAG based Chat with PDF\")`: This creates the main header displayed at the top of the app, indicating that the app is for Retrieval-Augmented Generation (RAG) based chat with PDFs.\n",
    "  \n",
    "  3. **User Input for Questions**:\n",
    "     - `user_question = st.text_input(\"Ask a Question from the PDF Files\")`: This creates a text input field where users can type in their questions related to the uploaded PDF files.\n",
    "     - If the user inputs a question, the `user_input` function is called to process the question and generate a response based on the content of the PDFs.\n",
    "\n",
    "  4. **PDF Upload and Processing**:\n",
    "     - The app includes a sidebar (`st.sidebar`) where users can upload their PDF files.\n",
    "     - `pdf_doc = st.file_uploader(\"Upload your PDF Files and Click on the Submit & Process Button\", accept_multiple_files=True)`: This file uploader widget allows users to upload multiple PDF files.\n",
    "     - If the user clicks the \"Submit & Process\" button, the app:\n",
    "       - Displays a spinner (`st.spinner(\"Processing...\")`) to indicate that the PDFs are being processed.\n",
    "       - Calls the `pdf_read` function to extract text from the uploaded PDFs.\n",
    "       - Uses the `get_chunks` function to split the extracted text into manageable chunks.\n",
    "       - Stores these chunks in a FAISS vector store via the `vector_store` function.\n",
    "       - Finally, notifies the user that the processing is complete with `st.success(\"Done\")`.\n",
    "\n",
    "- **Returns**:\n",
    "  - This function does not return any values but manages the user interface and interaction flow within the Streamlit app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the Streamlit app for interacting with PDF-based question answering.\n",
    "\n",
    "    This function sets up the Streamlit page configuration and layout, including headers, \n",
    "    user input fields, and file upload widgets. It handles user questions and processes\n",
    "    uploaded PDF files to enable question answering based on the content of the PDFs.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Configure the Streamlit page\n",
    "    st.set_page_config(page_title=\"Chat PDF\")\n",
    "    \n",
    "    # Set up the main header of the app\n",
    "    st.header(\"RAG based Chat with PDF\")\n",
    "    \n",
    "    # Create a text input field for users to ask questions related to the PDF files\n",
    "    user_question = st.text_input(\"Ask a Question from the PDF Files\")\n",
    "    \n",
    "    # If a question is provided, handle it by passing it to the `user_input` function\n",
    "    if user_question:\n",
    "        user_input(user_question)\n",
    "    \n",
    "    # Sidebar for uploading PDF files and processing them\n",
    "    with st.sidebar:\n",
    "        # File uploader widget for PDF files\n",
    "        pdf_doc = st.file_uploader(\"Upload your PDF Files and Click on the Submit & Process Button\", accept_multiple_files=True)\n",
    "        \n",
    "        # Button to trigger PDF processing\n",
    "        if st.button(\"Submit & Process\"):\n",
    "            with st.spinner(\"Processing...\"):\n",
    "                # Read and extract text from the uploaded PDF files\n",
    "                raw_text = pdf_read(pdf_doc)\n",
    "                \n",
    "                # Split the raw text into chunks\n",
    "                text_chunks = get_chunks(raw_text)\n",
    "                \n",
    "                # Store the chunks in a vector store for later retrieval\n",
    "                vector_store(text_chunks)\n",
    "                \n",
    "                # Notify the user that the processing is complete\n",
    "                st.success(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Running the Streamlit App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create and run the Streamlit app from the terminal, follow these steps:\n",
    "\n",
    "1. **Create a Python File**:\n",
    "   - Open your preferred code editor (e.g., VSCode, PyCharm, or a simple text editor).\n",
    "   - Copy all the code you have written in your Jupyter notebook, including the imports, function definitions (`pdf_read`, `get_chunks`, `vector_store`, `get_conversational_chain`, `user_input`, `main`), and any additional code.\n",
    "   - Paste the code into a new file and save it with the name `app.py`.\n",
    "\n",
    "2. **Add the Streamlit Run Command**:\n",
    "   - At the bottom of your `app.py` file, ensure you include the following check to run the `main` function when the script is executed:\n",
    "\n",
    "   ```python\n",
    "   if __name__ == \"__main__\":\n",
    "       main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
