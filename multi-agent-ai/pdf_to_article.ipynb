{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF to Blog Post Generator using Multi Agent AI\n",
    "\n",
    "This project automates the creation of blog posts from PDF documents using the OpenAI GPT-4 API. The system consists of multiple agents working together to extract text from a PDF, summarize the content, generate keywords, create an SEO-friendly title, and format the final blog post.\n",
    "\n",
    "## Workflow\n",
    "\n",
    "1. **PDF Extraction**: Extracts text from an uploaded PDF document.\n",
    "2. **Summarisation**: Breaks down the text into smaller chunks and generates summaries using GPT-4.\n",
    "3. **Keyword Extraction**: Extracts keywords and key topics from the text.\n",
    "4. **Title Generation**: Creates an engaging and SEO-optimized title for the blog post.\n",
    "5. **Blog Formatting**: Combines the title, summary, and keywords into a well-structured blog post.\n",
    "\n",
    "## Technology Stack\n",
    "\n",
    "- **OpenAI GPT-4**: Used for generating summaries, extracting keywords, and creating titles.\n",
    "- **PyPDF2**: Python library used to extract text from PDF files.\n",
    "- **CrewAI**: Coordinates the multi-agent system, allowing agents to work together efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the API key from environment variables\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PDF Extraction Agent\n",
    "\n",
    "This agent extracts the text from the PDF document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "class PDFExtractionAgent:\n",
    "    def extract_text(self, pdf_path):\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text()\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Summarisation Agent\n",
    "\n",
    "This agent splits the extracted text into smaller chunks and generates a summary for each chuck using GPT-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Set up OpenAI API key\n",
    "openai.api_key = api_key\n",
    "\n",
    "TOKEN_LIMIT = 2000  # Set token limit to avoid rate limits\n",
    "\n",
    "def split_text(text, max_tokens):\n",
    "    sentences = text.split('.')\n",
    "    chunks = []\n",
    "    current_chunk = ''\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk.split()) + len(sentence.split()) <= max_tokens:\n",
    "            current_chunk += sentence + '.'\n",
    "        else:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = sentence + '.'\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "class SummarizationAgent:\n",
    "    def generate_summary(self, text, chunk_size=TOKEN_LIMIT):\n",
    "        text_chunks = split_text(text, chunk_size)\n",
    "        summary_chunks = []\n",
    "\n",
    "        for chunk in text_chunks:\n",
    "            prompt = f\"Summarize the following text for a blog post:\\n\\n{chunk}\"\n",
    "            response = openai.chat.completions.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.7,\n",
    "                max_tokens=2000,\n",
    "            )\n",
    "            summary = response.choices[0].message.content\n",
    "            summary_chunks.append(summary)\n",
    "        \n",
    "        final_summary = ' '.join(summary_chunks)\n",
    "        \n",
    "        return final_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Keyword Agent\n",
    "\n",
    "This agent extracts keywords from the text using the GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeywordAgent:\n",
    "    def extract_keywords(self, text, chunk_size=TOKEN_LIMIT):\n",
    "        text_chunks = split_text(text, chunk_size)\n",
    "        keyword_chunks = []\n",
    "\n",
    "        for chunk in text_chunks:\n",
    "            prompt = f\"Extract keywords from the following text:\\n\\n{chunk}\"\n",
    "            response = openai.chat.completions.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.5,\n",
    "                max_tokens=200,\n",
    "            )\n",
    "            keywords = response.choices[0].message.content\n",
    "            keyword_chunks.append(keywords)\n",
    "        \n",
    "        final_keywords = ', '.join(keyword_chunks)\n",
    "        return final_keywords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Title Agent\n",
    "\n",
    "This agent generates title for blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitleAgent:\n",
    "    def generate_title(self, text, chunk_size=TOKEN_LIMIT):\n",
    "        # Use only the first chunk for title generation\n",
    "        text_chunk = split_text(text, chunk_size)[0]\n",
    "        prompt = f\"Generate an engaging title for a blog post based on this content:\\n\\n{text_chunk}\"\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=60,\n",
    "        )\n",
    "        title = response.choices[0].message.content\n",
    "        return title\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Blog Formatting Agent\n",
    "\n",
    "This agent combines the tile, summary, and keywords into a fromatted blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FormattingAgent:\n",
    "    def format_blog_post(self, title, summary, keywords):\n",
    "        blog_post = f\"# {title}\\n\\n{summary}\\n\\n**Keywords**: {keywords}\\n\"\n",
    "        return blog_post\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordinator\n",
    "\n",
    "The coordinator class manages all the agents, orchestrating the workflow from PDF extraction to blog post generation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlogPostGeneratorCoordinator:\n",
    "    def __init__(self):\n",
    "        self.pdf_agent = PDFExtractionAgent()\n",
    "        self.summary_agent = SummarizationAgent()\n",
    "        self.keyword_agent = KeywordAgent()\n",
    "        self.title_agent = TitleAgent()\n",
    "        self.formatting_agent = FormattingAgent()\n",
    "\n",
    "    def generate_blog_post_from_pdf(self, pdf_path):\n",
    "        # Agent 1: Extract text from PDF\n",
    "        pdf_text = self.pdf_agent.extract_text(pdf_path)\n",
    "\n",
    "        # Agent 2: Generate a summary for the blog post\n",
    "        summary = self.summary_agent.generate_summary(pdf_text)\n",
    "\n",
    "        # Agent 3: Extract keywords\n",
    "        keywords = self.keyword_agent.extract_keywords(pdf_text)\n",
    "\n",
    "        # Agent 4: Generate a title for the blog post\n",
    "        title = self.title_agent.generate_title(pdf_text)\n",
    "\n",
    "        # Agent 5: Format the blog post\n",
    "        blog_post = self.formatting_agent.format_blog_post(title, summary, keywords)\n",
    "\n",
    "        return blog_post\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"C:/Users/DanielGodden/Downloads/Generative-AI-and-LLMs-for-Dummies.pdf\"\n",
    "    coordinator = BlogPostGeneratorCoordinator()\n",
    "    blog_post = coordinator.generate_blog_post_from_pdf(pdf_path)\n",
    "\n",
    "    # Save blog post as text file (excluding keywords)\n",
    "    blog_post_without_keywords = blog_post.split(\"**Keywords**\")[0]  # Exclude keywords section\n",
    "    with open(\"generated_blog_post.txt\", \"w\") as text_file:\n",
    "        text_file.write(blog_post_without_keywords)\n",
    "        print(blog_post_without_keywords)\n",
    "\n",
    "    doc = Document()\n",
    "\n",
    "    lines = blog_post_without_keywords.split('\\n')\n",
    "    title = lines[0].strip(\"# \").strip()\n",
    "\n",
    "    doc.add_heading(title, 0)\n",
    "    doc.add_paragraph(blog_post_without_keywords)\n",
    "    doc.save(\"blog_post.docx\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
