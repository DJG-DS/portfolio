{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Web Scraping and Automation**\n",
    "\n",
    "The two scripts are designed to automate the process of interacting with websites and extracting data efficiently. \n",
    "\n",
    "1. **TripAdvisor-Specific Script**:\n",
    "   - This script focuses on scraping reviews and activity information from TripAdvisor for a given location. It navigates the website, handles dynamic content and pagination, and saves the extracted reviews into organized CSV files.\n",
    "   - While this script has been tested previously and shown to work effectively, recent changes in TripAdvisor's bot detection mechanisms have made it impossible to test currently.\n",
    "\n",
    "2. **Generalised Web Scraping Framework**:\n",
    "   - This script is a flexible framework designed to scrape content from any website by configuring key parameters like locators, navigation paths, and data extraction rules. It demonstrates reusable methods for automating web interactions and extracting data dynamically.\n",
    "   - While the generalised script has not been thoroughly tested, the functionality provided within it works and offers valuable insights into developing robust web scraping scripts.\n",
    "\n",
    "Both scripts illustrate practical approaches to automating website interactions and provide useful foundations for building custom web scraping solutions tailored to different needs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ethics of Web Scraping: Things to Consider**\n",
    "\n",
    "Web scraping can be a powerful tool for data collection, but it is essential to approach it responsibly and ethically. Below are key points to consider before starting:\n",
    "\n",
    "### **1. Understand the Website's Terms of Service**\n",
    "- Review the website's terms of service (ToS) to ensure scraping is allowed. Many websites explicitly prohibit automated data extraction.\n",
    "\n",
    "### **2. Avoid Overloading the Server**\n",
    "- Scrape responsibly by introducing delays between requests to avoid putting excessive load on the website's servers.\n",
    "\n",
    "### **3. Respect Copyright and Data Ownership**\n",
    "- Data on websites may be subject to copyright or other legal protections. Ensure you have the right to use the data you collect.\n",
    "\n",
    "### **4. Avoid Collecting Personal or Sensitive Information**\n",
    "- Never scrape personal, private, or sensitive information without explicit permission, as this may violate privacy laws like GDPR or CCPA.\n",
    "\n",
    "### **5. Identify Yourself Clearly**\n",
    "- Use a user-agent string that identifies your scraper instead of disguising it as a regular browser. This promotes transparency and good practices.\n",
    "\n",
    "### **6. Follow Local Laws and Regulations**\n",
    "- Web scraping laws vary by country. Ensure your activities comply with local legal frameworks to avoid legal repercussions.\n",
    "\n",
    "### **7. Test on Your Own Content**\n",
    "- Before scraping external websites, practice on your own or publicly available datasets to understand the techniques and minimise unintended consequences.\n",
    "\n",
    "### **8. Monitor for Changes in Website Structure**\n",
    "- Websites frequently update their structures, which could affect your scraper's functionality or flag it as suspicious behavior.\n",
    "\n",
    "### **9. Use APIs When Available**\n",
    "- If a website offers an API, prefer using it over scraping. APIs are typically designed to provide structured access to data while respecting the provider's terms.\n",
    "\n",
    "### **10. Seek Permission When Possible**\n",
    "- Contact the website's administrator to request permission for scraping, especially if your use case involves large-scale or repetitive data collection.\n",
    "\n",
    "By adhering to these principles, you can minimize ethical concerns and potential legal risks, promoting responsible web scraping practices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TripAdvisor Scraper: What the Script Does**\n",
    "\n",
    "This script automates the process of scraping reviews from TripAdvisor for a specified location. Below is a step-by-step explanation of its functionality:\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 1: Setup**\n",
    "1. **Imports Libraries**:\n",
    "   - The script imports essential libraries:\n",
    "     - `Selenium`: For web automation and interaction.\n",
    "     - `BeautifulSoup`: For parsing HTML content.\n",
    "     - `pandas`: For organizing and saving the scraped data.\n",
    "     - `os`: For file and directory handling.\n",
    "     - `random`: For introducing realistic delays.\n",
    "   \n",
    "2. **Configuration**:\n",
    "   - Defines the location to scrape reviews for (e.g., `Sandwell`).\n",
    "   - Sets up locators and parameters for interacting with TripAdvisor elements (e.g., buttons, search bar).\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 2: Navigate to TripAdvisor**\n",
    "1. **Open the Website**:\n",
    "   - The script navigates to the TripAdvisor homepage (`https://www.tripadvisor.co.uk/`).\n",
    "\n",
    "2. **Handle Cookie Consent**:\n",
    "   - If a cookie consent popup appears, the script automatically clicks the \"Accept\" button.\n",
    "\n",
    "3. **Search for a Location**:\n",
    "   - Uses the search bar on the homepage to input the location (e.g., `Sandwell`).\n",
    "   - Submits the search query.\n",
    "\n",
    "4. **Select the Desired Location**:\n",
    "   - Clicks on the first search result matching the location name.\n",
    "\n",
    "5. **Navigate to the \"Things to Do\" Section**:\n",
    "   - Once on the location's main page, the script clicks the \"Things to Do\" link.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 3: Extract Activity Data**\n",
    "1. **Load Activities**:\n",
    "   - Collects all activity links (e.g., attractions, places to visit) listed under \"Things to Do.\"\n",
    "   \n",
    "2. **Handle Pagination**:\n",
    "   - If multiple pages of activities are available, the script clicks the \"Next\" button to navigate through all pages.\n",
    "   - Continues until no more pages are available.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 4: Scrape Reviews**\n",
    "1. **Navigate to Each Activity**:\n",
    "   - Visits each activity's page using the collected links.\n",
    "\n",
    "2. **Extract Reviews**:\n",
    "   - Scrapes reviews from the activity page using `BeautifulSoup` for parsing.\n",
    "   - Handles dynamic content loading using Selenium.\n",
    "\n",
    "3. **Handle Pagination for Reviews**:\n",
    "   - If the activity has multiple review pages, the script clicks the \"Next\" button to navigate through all review pages.\n",
    "   - Collects reviews from all pages before moving to the next activity.\n",
    "\n",
    "4. **Clean and Format Reviews**:\n",
    "   - Removes TripAdvisor disclaimers and unnecessary content from reviews.\n",
    "   - Applies formatting to make the data cleaner and more readable.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 5: Save Data**\n",
    "1. **Organize Reviews**:\n",
    "   - Stores the scraped reviews for each activity in a list.\n",
    "\n",
    "2. **Save to CSV**:\n",
    "   - Writes the reviews into a CSV file.\n",
    "   - Each location has its own directory, and each activity's reviews are saved in a separate file.\n",
    "\n",
    "3. **Output Example**:\n",
    "   - Creates a directory structure like:\n",
    "     ```\n",
    "     scraped_data/tripadvisor/\n",
    "         Sandwell/\n",
    "             activity1.csv\n",
    "             activity2.csv\n",
    "     ```\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 6: Add Random Delays**\n",
    "1. **Mimic Human Behavior**:\n",
    "   - Introduces random delays between actions (e.g., navigating, clicking) to reduce the chance of being flagged as a bot.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 7: Error Handling**\n",
    "1. **Retries for Actions**:\n",
    "   - Implements retry logic for actions like clicking buttons or loading pages.\n",
    "   - If retries fail, the script skips the problematic step and continues.\n",
    "\n",
    "2. **Handle Failures Gracefully**:\n",
    "   - Logs errors and skips activities or locations that fail after multiple retries.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "The script automates the process of navigating TripAdvisor, extracting reviews, and saving them to a structured format. It is designed to handle dynamic content, pagination, and potential errors gracefully, making it flexible and robust for scraping multiple locations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import random  # For random delays\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# List of locations to scrape\n",
    "locations = [\"\"]\n",
    "\n",
    "def create_folder_if_not_exists(root_directory, folder_name):\n",
    "    \"\"\"\n",
    "    Creates a folder in the specified root directory if it doesn't exist.\n",
    "\n",
    "    Parameters:\n",
    "        root_directory (str): Path to the root directory.\n",
    "        folder_name (str): Name of the folder to create.\n",
    "\n",
    "    Returns:\n",
    "        str: Full path of the created or existing folder.\n",
    "    \"\"\"\n",
    "    folder_path = os.path.join(root_directory, folder_name)\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    return folder_path\n",
    "\n",
    "def initialize_driver():\n",
    "    \"\"\"\n",
    "    Initialises a Selenium WebDriver for Chrome.\n",
    "\n",
    "    Returns:\n",
    "        WebDriver: A configured instance of the Chrome WebDriver.\n",
    "    \"\"\"\n",
    "    return webdriver.Chrome()\n",
    "\n",
    "def random_delay(min_time=2, max_time=5):\n",
    "    \"\"\"\n",
    "    Introduces a random delay to mimic human browsing behavior.\n",
    "\n",
    "    Parameters:\n",
    "        min_time (int): Minimum delay time in seconds.\n",
    "        max_time (int): Maximum delay time in seconds.\n",
    "    \"\"\"\n",
    "    time.sleep(random.uniform(min_time, max_time))\n",
    "\n",
    "def retry_action(action, retries=3, wait_time=3):\n",
    "    \"\"\"\n",
    "    Retries a specified action if it fails, with a delay between retries.\n",
    "\n",
    "    Parameters:\n",
    "        action (function): The function to retry.\n",
    "        retries (int): Maximum number of retry attempts.\n",
    "        wait_time (int): Delay between retry attempts in seconds.\n",
    "\n",
    "    Returns:\n",
    "        The result of the successful action.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If the action fails after the maximum number of retries.\n",
    "    \"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            return action()\n",
    "        except Exception as e:\n",
    "            print(f\"Retry {attempt + 1}/{retries} failed: {e}\")\n",
    "            random_delay()\n",
    "    raise Exception(f\"Action failed after {retries} retries.\")\n",
    "\n",
    "def navigate_to_things_to_do(driver, location, max_retries=2):\n",
    "    \"\"\"\n",
    "    Navigates to the \"Things to Do\" page for a specified location.\n",
    "\n",
    "    Parameters:\n",
    "        driver (WebDriver): Selenium WebDriver instance.\n",
    "        location (str): Name of the location to search for.\n",
    "        max_retries (int): Maximum number of retries if navigation fails.\n",
    "\n",
    "    Returns:\n",
    "        str: URL of the \"Things to Do\" page.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If navigation fails after the maximum number of retries.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Open TripAdvisor homepage\n",
    "            driver.get(\"https://www.tripadvisor.co.uk/\")\n",
    "            random_delay()\n",
    "\n",
    "            # Accept cookies if prompted\n",
    "            retry_action(lambda: WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//button[contains(text(), 'Accept')]\"))\n",
    "            ).click())\n",
    "            random_delay()\n",
    "\n",
    "            # Search for the location\n",
    "            search_bar = retry_action(lambda: WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"input[placeholder='Where to?']\"))\n",
    "            ))\n",
    "            search_bar.send_keys(location)\n",
    "            search_bar.send_keys(Keys.RETURN)\n",
    "            random_delay()\n",
    "\n",
    "            # Click on the first search result\n",
    "            location_link = retry_action(lambda: WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//*[@id='BODY_BLOCK_JQUERY_REFLOW']/div[2]/div/div[2]/div/div/div/div/div[1]/div/div[1]/div/div[3]/div/div[1]/div/div[2]/div/div/div/div/div/div/div[2]/div/div[1]/span\"))\n",
    "            ))\n",
    "            location_link.click()\n",
    "            random_delay()\n",
    "\n",
    "            # Switch to the new tab opened\n",
    "            driver.switch_to.window(driver.window_handles[-1])\n",
    "            random_delay()\n",
    "\n",
    "            # Click the \"Things to Do\" button\n",
    "            things_to_do_button = retry_action(lambda: WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, \"#lithium-root > main > div.cBOoN > span > div > div > div > div:nth-child(2) > a > span\"))\n",
    "            ))\n",
    "            things_to_do_button.click()\n",
    "            random_delay()\n",
    "\n",
    "            return driver.current_url\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1}/{max_retries} failed for location {location}: {e}\")\n",
    "            driver.refresh()\n",
    "            random_delay()\n",
    "    raise Exception(f\"Failed to navigate to 'Things to Do' for {location} after {max_retries} retries.\")\n",
    "\n",
    "def scrape_reviews(driver, activity_url):\n",
    "    \"\"\"\n",
    "    Scrapes all reviews from a specified activity page.\n",
    "\n",
    "    Parameters:\n",
    "        driver (WebDriver): Selenium WebDriver instance.\n",
    "        activity_url (str): URL of the activity page.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of reviews scraped from the activity page.\n",
    "    \"\"\"\n",
    "    reviews = []\n",
    "    retry_action(lambda: driver.get(activity_url))\n",
    "    random_delay()\n",
    "\n",
    "    # Parse the page with BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    reviews += process_reviews(soup)\n",
    "\n",
    "    # Iterate through all review pages\n",
    "    while True:\n",
    "        try:\n",
    "            next_button = driver.find_element(By.XPATH, \"//a[@class='ui_button nav next primary']\")\n",
    "            retry_action(lambda: next_button.click())\n",
    "            random_delay()\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            reviews += process_reviews(soup)\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    return reviews\n",
    "\n",
    "def process_reviews(soup):\n",
    "    \"\"\"\n",
    "    Processes reviews from the BeautifulSoup-parsed HTML content.\n",
    "\n",
    "    Parameters:\n",
    "        soup (BeautifulSoup): Parsed HTML content of the page.\n",
    "\n",
    "    Returns:\n",
    "        list: A cleaned list of reviews.\n",
    "    \"\"\"\n",
    "    reviews = []\n",
    "    review_container_classname = '_c'\n",
    "    for review in soup.find_all('div', {'class': review_container_classname}):\n",
    "        # Remove unnecessary elements\n",
    "        class_to_drop = 'mwPje f M k'\n",
    "        elements_to_drop = review.find_all(class_=class_to_drop)\n",
    "        for element in elements_to_drop:\n",
    "            element.extract()\n",
    "\n",
    "        # Extract review text\n",
    "        review_text = review.find_next('div', {'class': review_container_classname})\n",
    "        if review_text:\n",
    "            reviews.append((review_text.text))\n",
    "\n",
    "    # Clean up reviews with regex\n",
    "    reviews = [re.sub(r'(\\d+)([A-Za-z]+)', r'\\1 \\2', item) for item in reviews]\n",
    "    reviews = [re.sub(r'([a-z])([A-Z])', r'\\1 \\2', item) for item in reviews]\n",
    "    reviews = [re.sub(r'\\.([A-Za-z])', r'. \\1', item) for item in reviews]\n",
    "\n",
    "    # Remove TripAdvisor's disclaimer text\n",
    "    subset_to_remove = \"This review is the subjective opinion of a Tripadvisor member and not of Tripadvisor LLC.\"\n",
    "    reviews = [review.replace(subset_to_remove, '').strip() for review in reviews]\n",
    "\n",
    "    # Filter out reviews before 2022\n",
    "    return remove_strings_with_numbers(reviews, 2000, 2021)\n",
    "\n",
    "def remove_strings_with_numbers(strings_list, start_range, end_range):\n",
    "    \"\"\"\n",
    "    Removes strings containing numbers within a specified range.\n",
    "\n",
    "    Parameters:\n",
    "        strings_list (list): List of strings to filter.\n",
    "        start_range (int): Start of the range.\n",
    "        end_range (int): End of the range.\n",
    "\n",
    "    Returns:\n",
    "        list: Filtered list of strings.\n",
    "    \"\"\"\n",
    "    filtered_strings = []\n",
    "    for string in strings_list:\n",
    "        contains_number = any(start_range <= int(word) <= end_range if word.isdigit() else False for word in string.split())\n",
    "        if not contains_number:\n",
    "            filtered_strings.append(string)\n",
    "    return filtered_strings\n",
    "\n",
    "def scrape_tripadvisor(locations):\n",
    "    \"\"\"\n",
    "    Main function to scrape TripAdvisor for specified locations.\n",
    "\n",
    "    Parameters:\n",
    "        locations (list): List of location names to scrape.\n",
    "    \"\"\"\n",
    "    root_directory = \"scraped_data/tripadvisor\"\n",
    "    driver = initialize_driver()\n",
    "\n",
    "    for location in locations:\n",
    "        # Create a folder for the location\n",
    "        location_folder = create_folder_if_not_exists(root_directory, location)\n",
    "        try:\n",
    "            # Navigate to \"Things to Do\" page\n",
    "            things_to_do_url = navigate_to_things_to_do(driver, location)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {location} due to error: {e}\")\n",
    "            continue\n",
    "\n",
    "        driver.get(things_to_do_url)\n",
    "        random_delay()\n",
    "\n",
    "        activity_urls = []\n",
    "        # Collect activity URLs\n",
    "        while True:\n",
    "            try:\n",
    "                activity_links = driver.find_elements(By.CSS_SELECTOR, \"a[href*='/Attraction_Review']\")\n",
    "                for link in activity_links:\n",
    "                    activity_urls.append(link.get_attribute('href'))\n",
    "\n",
    "                next_button = retry_action(lambda: driver.find_element(By.CSS_SELECTOR, \"a.next\"))\n",
    "                retry_action(lambda: next_button.click())\n",
    "                random_delay()\n",
    "            except:\n",
    "                break\n",
    "\n",
    "        # Scrape reviews for each activity\n",
    "        for activity_url in activity_urls:\n",
    "            try:\n",
    "                reviews = scrape_reviews(driver, activity_url)\n",
    "                reviews_df = pd.DataFrame(reviews, columns=[\"Review\"])\n",
    "                file_path = os.path.join(location_folder, f\"{activity_url.split('-')[-1]}.csv\")\n",
    "                reviews_df.to_csv(file_path, index=False)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to scrape reviews for {activity_url}: {e}\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "# Run the script\n",
    "scrape_tripadvisor(locations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Generalised Web Scraper: What the Script Does**\n",
    "\n",
    "This script is designed to scrape content from any website by using a configurable setup. Below is a step-by-step explanation of its functionality:\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 1: Setup**\n",
    "1. **Imports Libraries**:\n",
    "   - The script imports essential libraries:\n",
    "     - `Selenium`: For web automation and interaction.\n",
    "     - `BeautifulSoup`: For parsing HTML content.\n",
    "     - `pandas`: For organizing and saving the scraped data.\n",
    "     - `os`: For file and directory handling.\n",
    "     - `random`: For introducing realistic delays.\n",
    "   \n",
    "2. **Configuration**:\n",
    "   - Uses a dictionary (`config`) to define the scraping details for a specific website.\n",
    "   - The configuration includes:\n",
    "     - Base URL of the website.\n",
    "     - Element locators for cookies, search bars, target sections, and pagination.\n",
    "     - Search terms or keywords (if applicable).\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 2: Initialise and Configure WebDriver**\n",
    "1. **Initialize WebDriver**:\n",
    "   - Creates an instance of the Selenium WebDriver for browser automation.\n",
    "\n",
    "2. **Dynamic Delays**:\n",
    "   - Introduces random delays between actions to mimic human behavior and avoid detection as a bot.\n",
    "\n",
    "3. **Retry Mechanism**:\n",
    "   - Implements retry logic for critical actions like clicking buttons or loading pages.\n",
    "   - Allows multiple retries with delays before failing.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 3: Navigate to Target Section**\n",
    "1. **Open the Base URL**:\n",
    "   - Opens the website's homepage or base URL as defined in the `config`.\n",
    "\n",
    "2. **Handle Cookie Consent**:\n",
    "   - Automatically clicks the \"Accept\" button for cookie consent if a locator is provided in the `config`.\n",
    "\n",
    "3. **Perform Search (If Applicable)**:\n",
    "   - Inputs a search term (e.g., location, product) into the search bar using the provided selector.\n",
    "   - Submits the search query and waits for results to load.\n",
    "\n",
    "4. **Navigate to Target Section**:\n",
    "   - Clicks on the link or button leading to the desired section of the website (e.g., \"Things to Do\").\n",
    "   - Uses locators provided in the `config`.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 4: Extract Data**\n",
    "1. **Parse the Page**:\n",
    "   - Extracts content from the page using `BeautifulSoup` for HTML parsing.\n",
    "   - Finds elements based on the `content_container` and `content_class` specified in the `config`.\n",
    "\n",
    "2. **Collect Data**:\n",
    "   - Appends the extracted data (e.g., reviews, product details) to a list for further processing.\n",
    "\n",
    "3. **Clean Data**:\n",
    "   - Cleans the extracted content using regular expressions or other methods.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 5: Handle Pagination**\n",
    "1. **Navigate Through Pages**:\n",
    "   - Locates and clicks the \"Next\" button to navigate to subsequent pages.\n",
    "   - Continues until no more pages are available.\n",
    "\n",
    "2. **Extract Data from All Pages**:\n",
    "   - Collects and appends data from each page before moving to the next.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 6: Save Data**\n",
    "1. **Organize Data**:\n",
    "   - Structures the collected data into a format suitable for saving (e.g., a list of dictionaries).\n",
    "\n",
    "2. **Save to CSV**:\n",
    "   - Writes the data into a CSV file using `pandas`.\n",
    "   - The file name and directory are defined in the `config`.\n",
    "\n",
    "3. **Output Example**:\n",
    "   - Creates a directory structure like:\n",
    "     ```\n",
    "     scraped_data/\n",
    "         website_name/\n",
    "             target_section.csv\n",
    "     ```\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 7: Error Handling**\n",
    "1. **Retries for Actions**:\n",
    "   - Automatically retries actions like clicking buttons, loading elements, or navigating pages.\n",
    "\n",
    "2. **Graceful Failure**:\n",
    "   - Skips sections or pages that fail after multiple retries.\n",
    "   - Logs errors for debugging.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "The generalized script provides a flexible framework for scraping data from any website. By configuring the `config` dictionary with site-specific details, the script can handle:\n",
    "- Navigation flows.\n",
    "- Content extraction.\n",
    "- Pagination.\n",
    "- Data storage.\n",
    "\n",
    "It is designed to be robust and adaptable, making it suitable for a wide range of web scraping tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def create_folder(path):\n",
    "    \"\"\"\n",
    "    Create a folder if it does not exist.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "\n",
    "def initialize_driver():\n",
    "    \"\"\"\n",
    "    Initialise the Selenium WebDriver.\n",
    "    \"\"\"\n",
    "    return webdriver.Chrome()\n",
    "\n",
    "\n",
    "def random_delay(min_time=2, max_time=5):\n",
    "    \"\"\"\n",
    "    Introduce random delay to mimic human behavior.\n",
    "    \"\"\"\n",
    "    time.sleep(random.uniform(min_time, max_time))\n",
    "\n",
    "\n",
    "def retry_action(action, retries=3, wait_time=3):\n",
    "    \"\"\"\n",
    "    Retry an action with specified retries and delay.\n",
    "    \"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            return action()\n",
    "        except Exception as e:\n",
    "            print(f\"Retry {attempt + 1}/{retries} failed: {e}\")\n",
    "            random_delay(wait_time)\n",
    "    raise Exception(f\"Action failed after {retries} retries.\")\n",
    "\n",
    "\n",
    "def navigate_to_section(driver, config):\n",
    "    \"\"\"\n",
    "    Generic navigation function to handle searching and navigating to specific sections.\n",
    "\n",
    "    Parameters:\n",
    "        driver: Selenium WebDriver instance.\n",
    "        config: Dictionary containing navigation details (e.g., search URL, search box locator).\n",
    "    \n",
    "    Returns:\n",
    "        str: URL of the target section.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Open the base URL\n",
    "        driver.get(config[\"base_url\"])\n",
    "        random_delay()\n",
    "\n",
    "        # Accept cookies if applicable\n",
    "        if \"accept_cookies\" in config:\n",
    "            retry_action(lambda: WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, config[\"accept_cookies\"]))\n",
    "            ).click())\n",
    "            random_delay()\n",
    "\n",
    "        # Perform search if applicable\n",
    "        if \"search_box\" in config and \"search_term\" in config:\n",
    "            search_box = retry_action(lambda: WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, config[\"search_box\"]))\n",
    "            ))\n",
    "            search_box.send_keys(config[\"search_term\"])\n",
    "            search_box.send_keys(Keys.RETURN)\n",
    "            random_delay()\n",
    "\n",
    "        # Click on target section if applicable\n",
    "        if \"target_section\" in config:\n",
    "            retry_action(lambda: WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, config[\"target_section\"]))\n",
    "            ).click())\n",
    "            random_delay()\n",
    "\n",
    "        # Return the current URL\n",
    "        return driver.current_url\n",
    "    except Exception as e:\n",
    "        print(f\"Navigation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_data(driver, config):\n",
    "    \"\"\"\n",
    "    Generic data extraction function for parsing and extracting content.\n",
    "\n",
    "    Parameters:\n",
    "        driver: Selenium WebDriver instance.\n",
    "        config: Dictionary containing extraction details (e.g., content container).\n",
    "    \n",
    "    Returns:\n",
    "        list: Extracted data.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    try:\n",
    "        # Parse the page with BeautifulSoup\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        container_selector = config.get(\"content_container\", \"div\")\n",
    "        content_class = config.get(\"content_class\", None)\n",
    "\n",
    "        # Extract content based on the configuration\n",
    "        for element in soup.find_all(container_selector, {\"class\": content_class}):\n",
    "            data.append(element.text.strip())\n",
    "        \n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Data extraction failed: {e}\")\n",
    "        return data\n",
    "\n",
    "\n",
    "def handle_pagination(driver, config):\n",
    "    \"\"\"\n",
    "    Handle pagination and collect data from multiple pages.\n",
    "\n",
    "    Parameters:\n",
    "        driver: Selenium WebDriver instance.\n",
    "        config: Dictionary containing pagination details (e.g., next button locator).\n",
    "    \n",
    "    Returns:\n",
    "        list: Data collected from all pages.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    while True:\n",
    "        try:\n",
    "            # Extract data from the current page\n",
    "            page_data = extract_data(driver, config)\n",
    "            all_data.extend(page_data)\n",
    "\n",
    "            # Click on the next page button if applicable\n",
    "            if \"next_button\" in config:\n",
    "                next_button = retry_action(lambda: driver.find_element(By.XPATH, config[\"next_button\"]))\n",
    "                retry_action(lambda: next_button.click())\n",
    "                random_delay()\n",
    "            else:\n",
    "                break\n",
    "        except:\n",
    "            break\n",
    "    return all_data\n",
    "\n",
    "\n",
    "def scrape_website(config):\n",
    "    \"\"\"\n",
    "    Main function to scrape any website using the provided configuration.\n",
    "\n",
    "    Parameters:\n",
    "        config: Dictionary containing scraping details for the website.\n",
    "    \"\"\"\n",
    "    driver = initialize_driver()\n",
    "    output_dir = config.get(\"output_dir\", \"scraped_data\")\n",
    "    create_folder(output_dir)\n",
    "\n",
    "    # Navigate to the target section\n",
    "    target_url = navigate_to_section(driver, config)\n",
    "    if not target_url:\n",
    "        print(\"Failed to navigate to the target section.\")\n",
    "        driver.quit()\n",
    "        return\n",
    "\n",
    "    # Collect data from all pages\n",
    "    driver.get(target_url)\n",
    "    all_data = handle_pagination(driver, config)\n",
    "\n",
    "    # Save the data to a CSV file\n",
    "    output_file = os.path.join(output_dir, f\"{config['output_name']}.csv\")\n",
    "    pd.DataFrame(all_data, columns=[\"Content\"]).to_csv(output_file, index=False)\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "\n",
    "# Example configuration for TripAdvisor\n",
    "tripadvisor_config = {\n",
    "    \"base_url\": \"https://www.tripadvisor.co.uk/\",\n",
    "    \"accept_cookies\": \"//button[contains(text(), 'Accept')]\",\n",
    "    \"search_box\": \"input[placeholder='Where to?']\",\n",
    "    \"search_term\": \"\",\n",
    "    \"target_section\": \"//*[@id='BODY_BLOCK_JQUERY_REFLOW']/div[2]/div/div[2]/div/div/div/div/div[1]/div/div[1]/div/div[3]/div/div[1]/div/div[2]/div/div/div/div/div/div/div[2]/div/div[1]/span\",\n",
    "    \"content_container\": \"div\",\n",
    "    \"content_class\": \"_c\",\n",
    "    \"next_button\": \"//a[@class='ui_button nav next primary']\",\n",
    "    \"output_name\": \"tripadvisor_sandwell\",\n",
    "    \"output_dir\": \"scraped_data/tripadvisor\"\n",
    "}\n",
    "\n",
    "# Run the scraper\n",
    "scrape_website(tripadvisor_config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
