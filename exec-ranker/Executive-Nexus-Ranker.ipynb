{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "314dc0df",
   "metadata": {},
   "source": [
    "## Executive Matching Ranker - Full Code Explanation\n",
    "\n",
    "This script builds a **Learning-to-Rank model** to match executives to business opportunities using historical match data, executive attributes, and opportunity features. It leverages a **LightGBM Ranker** model trained with domain-specific features engineered from JSON-encoded and string-based metadata. Below is a detailed walkthrough of each section of the code.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Data Loading and Cleaning\n",
    "\n",
    "We load three CSVs:\n",
    "- `exec_roles.csv`: Contains executive-level metadata. Each row has an `exec_entity_id`, a `type`, and a value in either `json_value` or `string_value`.\n",
    "- `match.csv`: Historical match labels for which executives (`exec_entity_id`) were matched to which assignments (`assignment_id`), along with a binary `outcome` (1 = match, 0 = no match).\n",
    "- `opp.csv`: Metadata about each opportunity/assignment, including `assignment_id`, `industry`, `sectors`, `sub_sectors`, `scale`, and `country`.\n",
    "\n",
    "All `assignment_id` and `exec_entity_id` columns are cast to `Int64` to ensure consistent merging and avoid mismatches due to NaNs or mixed types.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Feature Engineering\n",
    "\n",
    "#### a. Exec Role Pivoting\n",
    "We transform the `exec_roles` file from long to wide format using `.pivot_table()`. This creates one row per `exec_entity_id` and spreads the `type` column into multiple columns (e.g. `json_value_sectors`, `string_value_hq_address`, etc.). The `aggfunc=\"first\"` ensures that for multiple rows with the same `(exec_entity_id, type)`, we take the first one.\n",
    "\n",
    "#### b. Feature Matching Functions\n",
    "The core function `build_features()` creates:\n",
    "- **Exact match flags**:\n",
    "  - `sector_match`: Whether the exec’s sectors match the opportunity’s.\n",
    "  - `country_match`: Whether the exec’s HQ matches the opportunity's country.\n",
    "  - `scale_match`: Whether the exec's business scale matches.\n",
    "\n",
    "- **Jaccard similarity scores**:\n",
    "  - `sector_jaccard`, `sub_sector_jaccard`, `industry_jaccard`: Compare exec and opportunity lists (e.g., sectors or industries) using the **Jaccard similarity**:  \n",
    "    $$ J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|} $$\n",
    "    This quantifies overlap in sets (e.g. shared industry tags).\n",
    "\n",
    "All comparisons use `ast.literal_eval` to parse JSON-like strings into Python lists.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Data Preparation for Learning to Rank\n",
    "\n",
    "We filter down to:\n",
    "- Only rows with no missing values in critical features and the outcome column.\n",
    "- Only `assignment_id`s that have at least one positive match (`outcome == 1`) — otherwise the ranker can't learn.\n",
    "\n",
    "We split the data into **training** and **testing** by randomly selecting ~10% of `assignment_id`s to be held out. We ensure no leakage by grouping the split by `assignment_id` — this is crucial since ranking is context-dependent per group.\n",
    "\n",
    "The LightGBM ranker expects:\n",
    "- `X_train`: Feature matrix.\n",
    "- `y_train`: Binary outcomes per row.\n",
    "- `group_train`: Number of rows per `assignment_id` — i.e., how many executives were evaluated for each opportunity.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. LightGBM Ranker Training\n",
    "\n",
    "We use `LGBMRanker`, a model designed for **Learning to Rank** (LTR) tasks using the **LambdaMART** algorithm. It optimizes ranking loss functions (e.g., NDCG) by learning which items (execs) should be ranked higher for each group (assignment).\n",
    "\n",
    "We pass:\n",
    "- `n_estimators=100`: Number of boosting rounds.\n",
    "- `force_col_wise=True`: Forces column-wise tree building to avoid auto-detection overhead.\n",
    "- `verbosity=-1`: Suppresses LightGBM’s warnings.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Evaluation with Precision@5\n",
    "\n",
    "We calculate **Precision@5**: for each `assignment_id`, we sort executives by predicted score and take the top 5. If *any* of them was actually a correct match (`outcome==1`), that counts as a hit. The mean hit rate over all groups gives Precision@5:\n",
    "- High Precision@5 = the model often includes the correct exec in the top-5 predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Prediction for a New Opportunity\n",
    "\n",
    "The function `rank_execs_for_new_opp()` lets us simulate ranking executives for a new opportunity:\n",
    "- For a given opportunity row, it pairs the opportunity with every exec.\n",
    "- It constructs the same feature columns as used during training.\n",
    "- It predicts a score for each exec-opportunity pair and returns the **top 10 ranked execs**.\n",
    "\n",
    "This simulates the **real-world use case** of recommending executives for a live opportunity.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Concepts Explained\n",
    "\n",
    "| Concept               | Explanation |\n",
    "|----------------------|-------------|\n",
    "| `LGBMRanker`         | A LightGBM model specialised for ranking tasks, optimised using pairwise ranking losses like LambdaRank. |\n",
    "| Jaccard Similarity    | Measures similarity between two sets. Important when comparing tag-like data (e.g., sectors or industries). |\n",
    "| Group-based Ranking  | Each opportunity is treated as a group. The model must learn to rank the correct execs highest within each group. |\n",
    "| Precision@5          | A ranking metric that checks if the correct result appears in the top 5 predictions per group. |\n",
    "\n",
    "---\n",
    "\n",
    "### Why This Approach?\n",
    "\n",
    "This ranking framework is well-suited for this type of matching problem because:\n",
    "- Traditional classifiers ignore the **contextual nature of matching** (which exec is best *for a specific assignment*).\n",
    "- Learning-to-Rank frameworks like LightGBM Ranker consider *relative ordering* of candidates within each assignment.\n",
    "- Feature engineering with **exact matches and fuzzy Jaccard scores** gives the model meaningful signals.\n",
    "- It’s scalable, explainable, and can easily be extended with embeddings or domain-specific metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d0991e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precision@5: 0.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DanielGodden\\AppData\\Local\\Temp\\ipykernel_14380\\4278955408.py:135: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: g.sort_values(\"score\", ascending=False).head(5))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMRanker\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# ------------------------\n",
    "# 1. Load and clean data\n",
    "# ------------------------\n",
    "\n",
    "# Load input CSVs\n",
    "exec_roles = pd.read_csv(\"exec_roles.csv\")  # Executive attributes\n",
    "match = pd.read_csv(\"match.csv\")            # Matches between execs and opportunities\n",
    "opp = pd.read_csv(\"opp.csv\")                # Opportunities\n",
    "\n",
    "# Ensure key IDs are properly typed\n",
    "match[\"assignment_id\"] = match[\"assignment_id\"].astype(\"Int64\")\n",
    "match[\"exec_entity_id\"] = match[\"exec_entity_id\"].astype(\"Int64\")\n",
    "opp[\"assignment_id\"] = opp[\"assignment_id\"].astype(\"Int64\")\n",
    "\n",
    "# Pivot exec_roles to a wide format: one row per exec with structured features\n",
    "exec_roles_wide = exec_roles.pivot_table(\n",
    "    index=\"exec_entity_id\", \n",
    "    columns=\"type\", \n",
    "    values=[\"json_value\", \"string_value\"], \n",
    "    aggfunc=\"first\"\n",
    ")\n",
    "exec_roles_wide.columns = [f\"{a}_{b}\" for a, b in exec_roles_wide.columns]\n",
    "exec_roles_wide = exec_roles_wide.reset_index()\n",
    "exec_roles_wide = exec_roles_wide.dropna(subset=[\"exec_entity_id\"])\n",
    "exec_roles_wide[\"exec_entity_id\"] = exec_roles_wide[\"exec_entity_id\"].astype(\"Int64\")\n",
    "\n",
    "# Merge matches with opportunities and exec features\n",
    "match_opp = pd.merge(match, opp, on=\"assignment_id\", how=\"left\")\n",
    "data = pd.merge(match_opp, exec_roles_wide, on=\"exec_entity_id\", how=\"left\")\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# 2. Feature Engineering\n",
    "# ------------------------\n",
    "\n",
    "def jaccard_sim(list1, list2):\n",
    "    \"\"\"\n",
    "    Compute Jaccard similarity between two stringified lists.\n",
    "    Used for comparing sectors, sub-sectors, and industries.\n",
    "    \"\"\"\n",
    "    if pd.isna(list1) or pd.isna(list2):\n",
    "        return 0\n",
    "    try:\n",
    "        set1 = set(ast.literal_eval(list1))\n",
    "        set2 = set(ast.literal_eval(list2))\n",
    "        return len(set1 & set2) / len(set1 | set2) if (set1 | set2) else 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def build_features(df):\n",
    "    \"\"\"\n",
    "    Generate binary and similarity-based features for each exec-opportunity pair.\n",
    "    \"\"\"\n",
    "    df[\"sector_match\"] = (df[\"json_value_sectors\"] == df[\"sectors\"]).astype(int)\n",
    "    df[\"country_match\"] = (df[\"string_value_hq_address\"] == df[\"country\"]).astype(int)\n",
    "    df[\"scale_match\"] = (df[\"string_value_scale\"] == df[\"scale\"]).astype(int)\n",
    "    df[\"sector_jaccard\"] = df.apply(lambda r: jaccard_sim(r.get(\"json_value_sectors\"), r.get(\"sectors\")), axis=1)\n",
    "    df[\"sub_sector_jaccard\"] = df.apply(lambda r: jaccard_sim(r.get(\"json_value_sub_sectors\"), r.get(\"sub_sectors\")), axis=1)\n",
    "    df[\"industry_jaccard\"] = df.apply(lambda r: jaccard_sim(r.get(\"json_value_industry\"), r.get(\"industry\")), axis=1)\n",
    "    return df\n",
    "\n",
    "# Build feature columns\n",
    "data = build_features(data)\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# 3. Prepare for ranking\n",
    "# ------------------------\n",
    "\n",
    "# Define features to use for ranking\n",
    "features = [\n",
    "    \"sector_match\", \"country_match\", \"scale_match\",\n",
    "    \"sector_jaccard\", \"sub_sector_jaccard\", \"industry_jaccard\"\n",
    "]\n",
    "\n",
    "# Drop rows with missing critical fields\n",
    "data = data.dropna(subset=features + [\"outcome\", \"assignment_id\"])\n",
    "data[\"outcome\"] = data[\"outcome\"].astype(int)\n",
    "\n",
    "# Keep only assignment_ids with at least one successful match\n",
    "valid_assignments = data.groupby(\"assignment_id\")[\"outcome\"].sum()\n",
    "valid_assignments = valid_assignments[valid_assignments > 0].index.tolist()\n",
    "data = data[data[\"assignment_id\"].isin(valid_assignments)]\n",
    "\n",
    "# Create holdout test set from a sample of assignment_ids\n",
    "assignment_ids = data[\"assignment_id\"].unique()\n",
    "test_ids = np.random.choice(assignment_ids, size=max(3, int(0.1 * len(assignment_ids))), replace=False)\n",
    "train_ids = [aid for aid in assignment_ids if aid not in test_ids]\n",
    "\n",
    "# Split the data\n",
    "train_df = data[data[\"assignment_id\"].isin(train_ids)].copy()\n",
    "test_df = data[data[\"assignment_id\"].isin(test_ids)].copy()\n",
    "\n",
    "# Prepare data for LightGBM ranker\n",
    "X_train = train_df[features]\n",
    "y_train = train_df[\"outcome\"]\n",
    "group_train = train_df.groupby(\"assignment_id\").size().values  # Number of items per group\n",
    "\n",
    "X_test = test_df[features]\n",
    "y_test = test_df[\"outcome\"]\n",
    "group_test = test_df.groupby(\"assignment_id\").size().values\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# 4. Train LightGBM Ranker\n",
    "# ------------------------\n",
    "\n",
    "ranker = LGBMRanker(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    verbosity=-1,  # Suppresses most warnings\n",
    "    force_col_wise=True  # Removes overhead message\n",
    ")\n",
    "\n",
    "ranker.fit(X_train, y_train, group=group_train)\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# 5. Evaluate Precision@5\n",
    "# ------------------------\n",
    "\n",
    "# Predict relevance scores for test set\n",
    "test_df[\"score\"] = ranker.predict(X_test)\n",
    "\n",
    "# Get top 5 predictions per assignment_id\n",
    "top_k = (\n",
    "    test_df.groupby(\"assignment_id\", group_keys=False)\n",
    "    .apply(lambda g: g.sort_values(\"score\", ascending=False).head(5))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Calculate average precision@5: proportion of groups where at least 1 top-5 item is a correct match\n",
    "precision_at_5 = top_k.groupby(\"assignment_id\")[\"outcome\"].max().mean()\n",
    "print(f\"\\nPrecision@5: {precision_at_5:.3f}\")\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# 6. Predict top execs for new opportunity\n",
    "# ------------------------\n",
    "\n",
    "def rank_execs_for_new_opp(new_opp_row, exec_table, model, features):\n",
    "    \"\"\"\n",
    "    Generate a ranked list of top executives for a given opportunity.\n",
    "\n",
    "    Parameters:\n",
    "        new_opp_row (Series): A single opportunity row.\n",
    "        exec_table (DataFrame): Wide-format executive features.\n",
    "        model: Trained ranking model.\n",
    "        features (list): List of feature column names.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Top 10 exec_entity_ids with predicted scores.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    # For each exec, combine with new opportunity fields\n",
    "    for _, exec_row in exec_table.iterrows():\n",
    "        combined = new_opp_row.copy()\n",
    "        for col in exec_row.index:\n",
    "            combined[f\"exec_{col}\"] = exec_row[col]\n",
    "\n",
    "        # Manually add needed fields\n",
    "        combined[\"json_value_sectors\"] = exec_row.get(\"json_value_sectors\")\n",
    "        combined[\"json_value_sub_sectors\"] = exec_row.get(\"json_value_sub_sectors\")\n",
    "        combined[\"json_value_industry\"] = exec_row.get(\"json_value_industry\")\n",
    "        combined[\"string_value_hq_address\"] = exec_row.get(\"string_value_hq_address\")\n",
    "        combined[\"string_value_scale\"] = exec_row.get(\"string_value_scale\")\n",
    "        combined[\"exec_entity_id\"] = exec_row.get(\"exec_entity_id\")\n",
    "        rows.append(combined)\n",
    "\n",
    "    # Build feature matrix and predict scores\n",
    "    pred_df = pd.DataFrame(rows)\n",
    "    pred_df = build_features(pred_df)\n",
    "    pred_df[\"score\"] = model.predict(pred_df[features])\n",
    "\n",
    "    return pred_df[[\"exec_entity_id\", \"score\"]].sort_values(\"score\", ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c286d68",
   "metadata": {},
   "source": [
    "### Opportunities to Improve the Model\n",
    "\n",
    "#### Better Models\n",
    "\n",
    "- **CatBoostRanker**: Handles categorical features natively; often better on tabular data with high-cardinality strings.\n",
    "- **XGBoostRanker**: Another strong baseline; can handle sparse features better in some cases.\n",
    "- **Neural Ranking Models**: E.g., DSSM, BERT-based cross encoders, especially if exec bios or long descriptions are available.\n",
    "- **Transformer Encoders**: Use textual similarity from `sentence-transformers` or `TF-IDF` embeddings for richer semantic similarity.\n",
    "\n",
    "#### Enhanced Features\n",
    "\n",
    "- **Embeddings**:\n",
    "  - Convert `sector`, `industry`, and `country` fields to dense vectors.\n",
    "  - Use cosine similarity instead of Jaccard.\n",
    "  \n",
    "- **Temporal features**:\n",
    "  - Time since last successful match for execs.\n",
    "  - Recency of opportunity posting.\n",
    "\n",
    "- **Exec activity rate**:\n",
    "  - How many times the exec has been selected.\n",
    "  - Success ratio per sector/industry.\n",
    "\n",
    "- **Interaction features**:\n",
    "  - Historical co-occurrence of sector + country + scale for success.\n",
    "\n",
    "#### Data Enrichment\n",
    "\n",
    "- Incorporate **textual bios**, **past job history**, or **skills tags** for execs.\n",
    "- Add **company descriptions** or **financial metrics** for opportunities.\n",
    "- Resolve mismatches in naming conventions (e.g., sector tags) with standardised vocab or NLP similarity.\n",
    "\n",
    "---\n",
    "\n",
    "### Model Performance & Data Issues\n",
    "\n",
    "- If you're seeing `best gain: -inf`, it means:\n",
    "  - Features may not have enough variance to create informative splits.\n",
    "  - Consider binning or embedding categorical values for better signal.\n",
    "- Small sample size or unbalanced `outcome` labels can reduce model learning.\n",
    "\n",
    "---\n",
    "\n",
    "### Productionising the Model (AWS + PostgreSQL stack)\n",
    "\n",
    "Let’s assume:\n",
    "\n",
    "- AWS is used for compute, deployment, monitoring\n",
    "- PostgreSQL stores execs, opportunities, and predictions\n",
    "\n",
    "#### System Architecture\n",
    "\n",
    "```text\n",
    "+-----------------+       +----------------+       +--------------------+\n",
    "| PostgreSQL DB   | <---> | ETL/Feature Job| --->  |    Model Training   |\n",
    "| (execs, opps)   |       | (Lambda/Airflow)|       | (SageMaker/EC2)     |\n",
    "+-----------------+       +----------------+       +--------------------+\n",
    "                                     |                       |\n",
    "                                     |                       v\n",
    "                               +--------------+     +---------------------+\n",
    "                               |   Model API  | <-- |  Model Registry     |\n",
    "                               | (FastAPI ECS)|     |  (MLflow/S3/DVC)    |\n",
    "                               +--------------+     +---------------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ae291d",
   "metadata": {},
   "source": [
    "## Steps to Productionise\n",
    "\n",
    "### 1. ETL Pipeline\n",
    "- Extract new execs, opportunities, and outcomes from **PostgreSQL** or **S3**\n",
    "- Compute features via **Airflow** or **Lambda** job\n",
    "- Save processed data to **S3** or a staging table\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Model Training\n",
    "- Run **nightly/weekly batch jobs** on **EC2** or **SageMaker**\n",
    "- Use **MLflow** for model versioning and hyperparameter tracking\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Model API\n",
    "- Deploy via **FastAPI** app on **AWS ECS** or **Lambda**\n",
    "- Expose `/predict` endpoint taking `assignment_id` and returning **top 10 `exec_entity_id`s**\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Predictions Table\n",
    "- Store `assignment_id`, `exec_entity_id`, `score`, `prediction_time` into **PostgreSQL**\n",
    "- Use for validation or feedback loop later\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Monitoring & Retraining\n",
    "- Use **CloudWatch** + custom metrics to track:\n",
    "  - Prediction volume\n",
    "  - Latency\n",
    "  - Precision@5 drift\n",
    "- Auto-trigger retraining when:\n",
    "  - New batch of labelled data arrives\n",
    "  - **Precision@5** degrades over time\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Model Retraining Workflow\n",
    "- Re-train on new labels **weekly or monthly**\n",
    "- Compare new vs old model performance\n",
    "- If better, **promote to production** (via MLflow or internal registry)\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluation Metrics To Track\n",
    "\n",
    "| Metric            | Purpose                                 |\n",
    "|-------------------|-----------------------------------------|\n",
    "| **Precision@5**   | Measures top-k match success            |\n",
    "| **MRR**           | Mean Reciprocal Rank – position of first relevant match |\n",
    "| **NDCG**          | Normalised Discounted Cumulative Gain – ranks correct results higher |\n",
    "| **Recall@K**      | Measures recall within top-k predictions|\n",
    "\n",
    "Use these metrics **over rolling windows (e.g. last 7 days)** to track live model performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Final Thoughts\n",
    "\n",
    "This ranking system is:\n",
    "\n",
    "- Simple to implement  \n",
    "- Scalable with minimal infrastructure  \n",
    "- Already delivers business insight via **Precision@5**\n",
    "\n",
    "With improvements such as:\n",
    "\n",
    "- More data\n",
    "- Embedding-based similarity\n",
    "- Regular retraining\n",
    "- Full MLOps integration\n",
    "\n",
    "…it can evolve into a production-grade, self-improving exec recommender system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd50e9f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
